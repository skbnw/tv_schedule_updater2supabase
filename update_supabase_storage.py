#!/usr/bin/env python3
"""
Supabaseã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã®JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ›´æ–°ã—ã¦å‡ºæ¼”è€…æƒ…å ±ã‚’è¿½åŠ ã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
07/19ã‚’ä¸­å¿ƒã«å‰å¾Œ6æ—¥åˆ†ï¼ˆ07/13ã€œ07/25ï¼‰ã‚’å¯¾è±¡
"""

import os
import json
import requests
import time
import random
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
from supabase import create_client, Client

# ç’°å¢ƒå¤‰æ•°ã‹ã‚‰è¨­å®šã‚’å–å¾—
def get_env(key, default=None):
    v = os.environ.get(key)
    if v is None:
        return default
    return v

# Supabaseè¨­å®š
SUPABASE_URL = get_env("SUPABASE_URL")
SUPABASE_KEY = get_env("SUPABASE_KEY")

if not SUPABASE_URL or not SUPABASE_KEY:
    print("âŒ Supabaseç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
    print("SUPABASE_URL ã¨ SUPABASE_KEY ã‚’è¨­å®šã—ã¦ãã ã•ã„")
    print("ä¾‹: $env:SUPABASE_URL='https://your-project.supabase.co'")
    print("ä¾‹: $env:SUPABASE_KEY='your-service-role-key'")
    exit(1)

supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)

def clean_text(text):
    """ãƒ†ã‚­ã‚¹ãƒˆã®ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°"""
    if text is None:
        return ""
    return str(text).strip()

def safe_extract_talent_info(link_element):
    """å®‰å…¨ãªã‚¿ãƒ¬ãƒ³ãƒˆæƒ…å ±æŠ½å‡º"""
    try:
        name = clean_text(link_element.get_text(strip=True))
        href = link_element.get("href", "")
        
        if not name or not href:
            return None
        
        if href.rstrip("/").endswith("talents"):
            return None
            
        talent_id = href.rstrip("/").split("/")[-1].split("?")[0]
        if not talent_id.isdigit():
            return None
            
        full_link = "https://bangumi.org" + href if href.startswith("/") else href
        
        return {
            "talent_id": talent_id,
            "name": name,
            "link": full_link
        }
    except Exception as e:
        print(f"âš ï¸ ã‚¿ãƒ¬ãƒ³ãƒˆæƒ…å ±æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
        return None

def extract_performers_from_html(soup_detail):
    """HTMLã‹ã‚‰å‡ºæ¼”è€…æƒ…å ±ã‚’æŠ½å‡ºï¼ˆæ”¹è‰¯ç‰ˆï¼‰"""
    performers = {}
    
    try:
        # æ–¹æ³•1: ul.additionå†…ã®å‡ºæ¼”è€…æƒ…å ±ã‚’å–å¾—
        addition_section = soup_detail.find("ul", class_="addition")
        if addition_section:
            performer_text = addition_section.get_text(strip=True)
            print(f"  ğŸ” å‡ºæ¼”è€…ãƒ†ã‚­ã‚¹ãƒˆæ¤œå‡º: {performer_text[:100]}...")
            
            performer_links = addition_section.find_all("a", href=lambda x: x and "/talents/" in x)
            for link in performer_links:
                talent_info = safe_extract_talent_info(link)
                if talent_info:
                    performers[talent_info["name"]] = talent_info["link"]
        
        # æ–¹æ³•2: ul.talent_panelå†…ã®ã‚¿ãƒ¬ãƒ³ãƒˆæƒ…å ±ã‚’å–å¾—
        talent_panel = soup_detail.find("ul", class_="talent_panel")
        if talent_panel:
            talent_links = talent_panel.find_all("a", href=lambda x: x and "/talents/" in x)
            for link in talent_links:
                talent_info = safe_extract_talent_info(link)
                if talent_info:
                    performers[talent_info["name"]] = talent_info["link"]
        
        # æ–¹æ³•3: ãƒšãƒ¼ã‚¸å…¨ä½“ã‹ã‚‰ã‚¿ãƒ¬ãƒ³ãƒˆãƒªãƒ³ã‚¯ã‚’å–å¾—ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
        if not performers:
            all_talent_links = soup_detail.find_all("a", href=lambda x: x and "/talents/" in x)
            for link in all_talent_links:
                talent_info = safe_extract_talent_info(link)
                if talent_info:
                    performers[talent_info["name"]] = talent_info["link"]
        
        print(f"  ğŸ‘¥ å‡ºæ¼”è€…æ¤œå‡º: {len(performers)}å")
        if performers:
            for name in list(performers.keys())[:3]:
                print(f"    - {name}")
            if len(performers) > 3:
                print(f"    ... ä»–{len(performers) - 3}å")
        
    except Exception as e:
        print(f"âš ï¸ å‡ºæ¼”è€…æƒ…å ±æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
    
    return performers

# JSONãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸å
STORAGE_BUCKET = "json-backups"

def get_storage_files(target_dates):
    """æŒ‡å®šã•ã‚ŒãŸæ—¥ä»˜ã®Supabaseã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ã‚’å–å¾—"""
    files = []
    
    for date_str in target_dates:
        try:
            # æ—¥ä»˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®ãƒãƒ£ãƒ³ãƒãƒ«ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä¸€è¦§ã‚’å–å¾—
            channel_dirs = supabase.storage.from_(STORAGE_BUCKET).list(path=date_str)
            
            if channel_dirs:
                for ch_dir in channel_dirs:
                    # typeãŒ'folder'ã§ãªãã¦ã‚‚ã€ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¨ã—ã¦æ‰±ã†
                    ch_path = f"{date_str}/{ch_dir.get('name', '')}"
                    
                    # ãƒãƒ£ãƒ³ãƒãƒ«ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®JSONãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ã‚’å–å¾—
                    json_files = supabase.storage.from_(STORAGE_BUCKET).list(path=ch_path)
                    
                    for jf in json_files:
                        if jf.get('name', '').endswith('.json'):
                            file_path = f"{ch_path}/{jf['name']}"
                            files.append(file_path)
                            print(f"  ğŸ“„ ç™ºè¦‹: {file_path}")
            
        except Exception as e:
            print(f"âš ï¸ {date_str}ã®ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
    
    return files

def download_and_update_json(file_path):
    """JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦å‡ºæ¼”è€…æƒ…å ±ã‚’æ›´æ–°"""
    try:
        print(f"\nğŸ“„ å‡¦ç†ä¸­: {file_path}")
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        response = supabase.storage.from_(STORAGE_BUCKET).download(file_path)
        data = json.loads(response.decode('utf-8'))
        
        print(f"  ğŸ“º ç•ªçµ„: {data.get('program_title', 'ä¸æ˜')}")
        print(f"  ğŸ“… æ”¾é€æ—¥: {data.get('broadcast_date', 'ä¸æ˜')}")
        
        # æ—¢ã«å‡ºæ¼”è€…æƒ…å ±ãŒã‚ã‚‹å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
        if data.get('performers') and len(data['performers']) > 0:
            print(f"  âœ… æ—¢ã«å‡ºæ¼”è€…æƒ…å ±ãŒã‚ã‚Šã¾ã™ï¼ˆ{len(data['performers'])}åï¼‰")
            return False, "æ—¢å­˜ãƒ‡ãƒ¼ã‚¿"
        
        # ç•ªçµ„è©³ç´°ãƒšãƒ¼ã‚¸ã‹ã‚‰å‡ºæ¼”è€…æƒ…å ±ã‚’å–å¾—
        event_id = data.get('event_id')
        if not event_id:
            print(f"  âŒ event_idãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return False, "event_idãªã—"
        
        # è¤‡æ•°ã®URLãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è©¦ã™
        url_patterns = [
            f"https://bangumi.org/tv_events/seasons?season_id={event_id}",
            f"https://bangumi.org/tv_events/seasons?season_id={event_id}&from=x",
            f"https://bangumi.org/tv_events/seasons?season_id={event_id}&from=fb",
            f"https://bangumi.org/tv_events/{event_id}",
            f"https://bangumi.org/tv_events/seasons/{event_id}",
        ]
        
        performers_found = False
        
        for url in url_patterns:
            print(f"  ğŸ”— è©¦è¡Œ: {url}")
            
            try:
                res = requests.get(url, timeout=30)
                if res.status_code == 200:
                    soup = BeautifulSoup(res.text, 'html.parser')
                    
                    print(f"  ğŸ“„ ãƒšãƒ¼ã‚¸ã‚µã‚¤ã‚º: {len(res.text)}æ–‡å­—")
                    
                    # å‡ºæ¼”è€…æƒ…å ±ã‚’æŠ½å‡º
                    performer_links = extract_performers_from_html(soup)
                    
                    if performer_links:
                        # å‡ºæ¼”è€…æƒ…å ±ã‚’JSONã«è¿½åŠ 
                        performers_data = []
                        for name, link in performer_links.items():
                            talent_id = link.rstrip("/").split("/")[-1].split("?")[0]
                            performers_data.append({
                                "talent_id": talent_id,
                                "name": name,
                                "link": link
                            })
                        
                        data['performers'] = performers_data
                        data['performer_count'] = len(performers_data)
                        data['updated_at'] = datetime.now().isoformat()
                        
                        # æ›´æ–°ã•ã‚ŒãŸJSONã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
                        json_bytes = json.dumps(data, ensure_ascii=False, indent=2).encode('utf-8')
                        supabase.storage.from_(STORAGE_BUCKET).update(file_path, json_bytes)
                        
                        print(f"  âœ… å‡ºæ¼”è€…æƒ…å ±ã‚’è¿½åŠ ã—ã¾ã—ãŸï¼ˆ{len(performers_data)}åï¼‰")
                        performers_found = True
                        break
                    else:
                        print(f"  âš ï¸ å‡ºæ¼”è€…æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                else:
                    print(f"  âŒ ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: {res.status_code}")
                    
            except Exception as e:
                print(f"  âŒ ã‚¨ãƒ©ãƒ¼: {e}")
                continue
        
        if performers_found:
            return True, "æ›´æ–°æˆåŠŸ"
        else:
            return False, "å‡ºæ¼”è€…æƒ…å ±ãªã—"
            
    except Exception as e:
        print(f"âŒ JSONãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
        return False, f"ã‚¨ãƒ©ãƒ¼: {e}"

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("ğŸš€ Supabaseã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã®å‡ºæ¼”è€…æƒ…å ±æ›´æ–°ã‚’é–‹å§‹ã—ã¾ã™")
    
    # 07/19ã‚’ä¸­å¿ƒã«å‰å¾Œ6æ—¥åˆ†ã®æ—¥ä»˜ã‚’ç”Ÿæˆ
    center_date = datetime(2025, 7, 19)
    target_dates = []
    
    for i in range(-6, 7):  # -6æ—¥ã‹ã‚‰+6æ—¥
        target_date = center_date + timedelta(days=i)
        target_dates.append(target_date.strftime('%Y-%m-%d'))
    
    print(f"ğŸ“… å¯¾è±¡æœŸé–“: {target_dates[0]} ã€œ {target_dates[-1]}")
    print(f"ğŸ“‹ å¯¾è±¡æ—¥æ•°: {len(target_dates)}æ—¥")
    
    # ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ã‚’å–å¾—
    print("\nğŸ“‚ ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ã‚’å–å¾—ä¸­...")
    storage_files = get_storage_files(target_dates)
    
    if not storage_files:
        print("âŒ æ›´æ–°å¯¾è±¡ã®JSONãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return
    
    print(f"\nğŸ“‹ æ›´æ–°å¯¾è±¡: {len(storage_files)}ãƒ•ã‚¡ã‚¤ãƒ«")
    
    updated_count = 0
    error_count = 0
    skipped_count = 0
    
    for file_path in storage_files:
        try:
            success, status = download_and_update_json(file_path)
            
            if success:
                updated_count += 1
            elif status == "æ—¢å­˜ãƒ‡ãƒ¼ã‚¿":
                skipped_count += 1
            else:
                error_count += 1
            
            # ã‚µãƒ¼ãƒãƒ¼ã«è² è·ã‚’ã‹ã‘ãªã„ã‚ˆã†å°‘ã—å¾…æ©Ÿ
            time.sleep(random.uniform(2, 4))
            
        except Exception as e:
            print(f"âŒ {file_path} ã®å‡¦ç†ã§ã‚¨ãƒ©ãƒ¼: {e}")
            error_count += 1
    
    print(f"\nğŸ“Š æ›´æ–°å®Œäº†:")
    print(f"  âœ… æ›´æ–°æˆåŠŸ: {updated_count}ä»¶")
    print(f"  â­ï¸ ã‚¹ã‚­ãƒƒãƒ—: {skipped_count}ä»¶")
    print(f"  âŒ ã‚¨ãƒ©ãƒ¼: {error_count}ä»¶")
    print(f"  ğŸ“‹ ç·å‡¦ç†ä»¶æ•°: {len(storage_files)}ä»¶")

if __name__ == '__main__':
    main() 