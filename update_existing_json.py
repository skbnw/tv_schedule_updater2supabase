#!/usr/bin/env python3
"""
æ—¢å­˜ã®JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ›´æ–°ã—ã¦å‡ºæ¼”è€…æƒ…å ±ã‚’è¿½åŠ ã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""

import os
import json
import requests
import time
import random
from bs4 import BeautifulSoup
from datetime import datetime

def clean_text(text):
    """ãƒ†ã‚­ã‚¹ãƒˆã®ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°"""
    if text is None:
        return ""
    return str(text).strip()

def safe_extract_talent_info(link_element):
    """å®‰å…¨ãªã‚¿ãƒ¬ãƒ³ãƒˆæƒ…å ±æŠ½å‡º"""
    try:
        name = clean_text(link_element.get_text(strip=True))
        href = link_element.get("href", "")
        
        if not name or not href:
            return None
        
        if href.rstrip("/").endswith("talents"):
            return None
            
        talent_id = href.rstrip("/").split("/")[-1].split("?")[0]
        if not talent_id.isdigit():
            return None
            
        full_link = "https://bangumi.org" + href if href.startswith("/") else href
        
        return {
            "talent_id": talent_id,
            "name": name,
            "link": full_link
        }
    except Exception as e:
        print(f"âš ï¸ ã‚¿ãƒ¬ãƒ³ãƒˆæƒ…å ±æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
        return None

def extract_performers_from_html(soup_detail):
    """HTMLã‹ã‚‰å‡ºæ¼”è€…æƒ…å ±ã‚’æŠ½å‡ºï¼ˆæ”¹è‰¯ç‰ˆï¼‰"""
    performers = {}
    
    try:
        # æ–¹æ³•1: ul.additionå†…ã®å‡ºæ¼”è€…æƒ…å ±ã‚’å–å¾—
        addition_section = soup_detail.find("ul", class_="addition")
        if addition_section:
            performer_text = addition_section.get_text(strip=True)
            print(f"  ğŸ” å‡ºæ¼”è€…ãƒ†ã‚­ã‚¹ãƒˆæ¤œå‡º: {performer_text[:100]}...")
            
            performer_links = addition_section.find_all("a", href=lambda x: x and "/talents/" in x)
            for link in performer_links:
                talent_info = safe_extract_talent_info(link)
                if talent_info:
                    performers[talent_info["name"]] = talent_info["link"]
        
        # æ–¹æ³•2: ul.talent_panelå†…ã®ã‚¿ãƒ¬ãƒ³ãƒˆæƒ…å ±ã‚’å–å¾—
        talent_panel = soup_detail.find("ul", class_="talent_panel")
        if talent_panel:
            talent_links = talent_panel.find_all("a", href=lambda x: x and "/talents/" in x)
            for link in talent_links:
                talent_info = safe_extract_talent_info(link)
                if talent_info:
                    performers[talent_info["name"]] = talent_info["link"]
        
        # æ–¹æ³•3: ãƒšãƒ¼ã‚¸å…¨ä½“ã‹ã‚‰ã‚¿ãƒ¬ãƒ³ãƒˆãƒªãƒ³ã‚¯ã‚’å–å¾—ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
        if not performers:
            all_talent_links = soup_detail.find_all("a", href=lambda x: x and "/talents/" in x)
            for link in all_talent_links:
                talent_info = safe_extract_talent_info(link)
                if talent_info:
                    performers[talent_info["name"]] = talent_info["link"]
        
        print(f"  ğŸ‘¥ å‡ºæ¼”è€…æ¤œå‡º: {len(performers)}å")
        if performers:
            for name in list(performers.keys())[:3]:
                print(f"    - {name}")
            if len(performers) > 3:
                print(f"    ... ä»–{len(performers) - 3}å")
        
    except Exception as e:
        print(f"âš ï¸ å‡ºæ¼”è€…æƒ…å ±æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
    
    return performers

def update_json_file(json_file_path):
    """JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ›´æ–°ã—ã¦å‡ºæ¼”è€…æƒ…å ±ã‚’è¿½åŠ """
    try:
        # JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
        with open(json_file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        print(f"ğŸ“„ å‡¦ç†ä¸­: {json_file_path}")
        print(f"  ğŸ“º ç•ªçµ„: {data.get('program_title', 'ä¸æ˜')}")
        print(f"  ğŸ“… æ”¾é€æ—¥: {data.get('broadcast_date', 'ä¸æ˜')}")
        
        # æ—¢ã«å‡ºæ¼”è€…æƒ…å ±ãŒã‚ã‚‹å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
        if data.get('performers') and len(data['performers']) > 0:
            print(f"  âœ… æ—¢ã«å‡ºæ¼”è€…æƒ…å ±ãŒã‚ã‚Šã¾ã™ï¼ˆ{len(data['performers'])}åï¼‰")
            return False
        
        # ç•ªçµ„è©³ç´°ãƒšãƒ¼ã‚¸ã‹ã‚‰å‡ºæ¼”è€…æƒ…å ±ã‚’å–å¾—
        event_id = data.get('event_id')
        if not event_id:
            print(f"  âŒ event_idãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return False
        
        # æ­£ã—ã„URLå½¢å¼ã‚’ç”Ÿæˆï¼ˆEPGãƒšãƒ¼ã‚¸ã®hrefå½¢å¼ã«åŸºã¥ãï¼‰
        url = f"https://bangumi.org/tv_events/seasons?season_id={event_id}&from=x"
        print(f"  ğŸ”— URL: {url}")
        
        try:
            res = requests.get(url, timeout=30)
            res.raise_for_status()
            soup = BeautifulSoup(res.text, 'html.parser')
            
            print(f"  ğŸ“„ ãƒšãƒ¼ã‚¸ã‚µã‚¤ã‚º: {len(res.text)}æ–‡å­—")
            
            # å‡ºæ¼”è€…æƒ…å ±ã‚’æŠ½å‡º
            performer_links = extract_performers_from_html(soup)
            
            if performer_links:
                # å‡ºæ¼”è€…æƒ…å ±ã‚’JSONã«è¿½åŠ 
                performers_data = []
                for name, link in performer_links.items():
                    talent_id = link.rstrip("/").split("/")[-1].split("?")[0]
                    performers_data.append({
                        "talent_id": talent_id,
                        "name": name,
                        "link": link
                    })
                
                data['performers'] = performers_data
                data['performer_count'] = len(performers_data)
                data['updated_at'] = datetime.now().isoformat()
                
                # JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ›´æ–°
                with open(json_file_path, 'w', encoding='utf-8') as f:
                    json.dump(data, f, ensure_ascii=False, indent=2)
                
                print(f"  âœ… å‡ºæ¼”è€…æƒ…å ±ã‚’è¿½åŠ ã—ã¾ã—ãŸï¼ˆ{len(performers_data)}åï¼‰")
                return True
            else:
                print(f"  âš ï¸ å‡ºæ¼”è€…æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                return False
                
        except Exception as e:
            print(f"  âŒ ãƒšãƒ¼ã‚¸å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return False
            
    except Exception as e:
        print(f"âŒ JSONãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
        return False

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("ğŸš€ æ—¢å­˜JSONãƒ•ã‚¡ã‚¤ãƒ«ã®å‡ºæ¼”è€…æƒ…å ±æ›´æ–°ã‚’é–‹å§‹ã—ã¾ã™")
    
    # ç¾åœ¨ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
    json_files = [f for f in os.listdir('.') if f.endswith('.json') and 'NHKG-TKY' in f]
    
    if not json_files:
        print("âŒ æ›´æ–°å¯¾è±¡ã®JSONãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return
    
    print(f"ğŸ“‹ æ›´æ–°å¯¾è±¡: {len(json_files)}ãƒ•ã‚¡ã‚¤ãƒ«")
    
    updated_count = 0
    error_count = 0
    
    for json_file in json_files:
        try:
            if update_json_file(json_file):
                updated_count += 1
            else:
                error_count += 1
            
            # ã‚µãƒ¼ãƒãƒ¼ã«è² è·ã‚’ã‹ã‘ãªã„ã‚ˆã†å°‘ã—å¾…æ©Ÿ
            time.sleep(random.uniform(2, 4))
            
        except Exception as e:
            print(f"âŒ {json_file} ã®å‡¦ç†ã§ã‚¨ãƒ©ãƒ¼: {e}")
            error_count += 1
    
    print(f"\nğŸ“Š æ›´æ–°å®Œäº†:")
    print(f"  âœ… æ›´æ–°æˆåŠŸ: {updated_count}ä»¶")
    print(f"  âŒ ã‚¨ãƒ©ãƒ¼: {error_count}ä»¶")
    print(f"  ğŸ“‹ ç·å‡¦ç†ä»¶æ•°: {len(json_files)}ä»¶")

if __name__ == '__main__':
    main() 