#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import json
import requests
from bs4 import BeautifulSoup
from supabase import create_client, Client

# ç’°å¢ƒå¤‰æ•°ã‹ã‚‰è¨­å®šã‚’å–å¾—
def get_env_var(var_name, default=None):
    """ç’°å¢ƒå¤‰æ•°ã‚’å–å¾—"""
    return os.getenv(var_name, default)

# Supabaseè¨­å®š
SUPABASE_URL = get_env_var('SUPABASE_URL')
SUPABASE_KEY = get_env_var('SUPABASE_KEY')

if not SUPABASE_URL or not SUPABASE_KEY:
    print("âŒ ç’°å¢ƒå¤‰æ•° SUPABASE_URL ã¾ãŸã¯ SUPABASE_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
    exit(1)

supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)

def find_sunday_discussion():
    """07/13ã®æ—¥æ›œè¨Žè«–ã‚’æ¤œç´¢"""
    print("ðŸ” 07/13ã®æ—¥æ›œè¨Žè«–ã‚’æ¤œç´¢ä¸­...")
    
    try:
        # NHKG-TKYãƒãƒ£ãƒ³ãƒãƒ«ã®ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ã‚’å–å¾—
        ch_path = "2025-07-13/NHKG-TKY"
        print(f"ðŸ“‚ ãƒ‘ã‚¹: {ch_path}")
        
        files = supabase.storage.from_("json-backups").list(path=ch_path)
        print(f"ðŸ“„ ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(files)}")
        
        if not files:
            print("âŒ ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            return
        
        sunday_discussion_files = []
        
        for file_info in files:
            if file_info.get('name', '').endswith('.json'):
                try:
                    file_path = f"{ch_path}/{file_info['name']}"
                    response = supabase.storage.from_("json-backups").download(file_path)
                    data = json.loads(response.decode('utf-8'))
                    
                    program_title = data.get('program_title', '')
                    if 'æ—¥æ›œè¨Žè«–' in program_title:
                        sunday_discussion_files.append({
                            'file': file_info['name'],
                            'data': data
                        })
                        print(f"ðŸ“º ç™ºè¦‹: {file_info['name']}")
                        print(f"   ã‚¿ã‚¤ãƒˆãƒ«: {program_title}")
                        
                except Exception as e:
                    print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        
        if sunday_discussion_files:
            print(f"\nðŸ“Š æ—¥æ›œè¨Žè«–ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(sunday_discussion_files)}")
            
            for i, program in enumerate(sunday_discussion_files):
                print(f"\n=== {i+1}ç•ªç›®ã®ãƒ•ã‚¡ã‚¤ãƒ« ===")
                print(f"ãƒ•ã‚¡ã‚¤ãƒ«: {program['file']}")
                print(f"ç•ªçµ„ã‚¿ã‚¤ãƒˆãƒ«: {program['data'].get('program_title', 'ä¸æ˜Ž')}")
                print(f"å‡ºæ¼”è€…æ•°: {len(program['data'].get('performers', []))}")
                
                performers = program['data'].get('performers', [])
                if performers:
                    print("å‡ºæ¼”è€…è©³ç´°:")
                    for j, performer in enumerate(performers):
                        print(f"  {j+1}. {performer.get('name', 'ä¸æ˜Ž')} (å½¹: {performer.get('role', 'ä¸æ˜Ž')})")
                else:
                    print("âš ï¸ å‡ºæ¼”è€…æƒ…å ±ãªã—")
                
                # event_idã‚’å–å¾—ã—ã¦å†ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’è©¦è¡Œ
                event_id = program['data'].get('event_id')
                if event_id:
                    print(f"\nðŸ”„ event_id: {event_id} ã§å†ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’è©¦è¡Œ...")
                    retry_scraping(event_id, program['data'].get('program_title', ''))
                
        else:
            print("âŒ æ—¥æ›œè¨Žè«–ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            
            # ãã®æ—¥ã®å…¨ç•ªçµ„ã‚¿ã‚¤ãƒˆãƒ«ã‚’è¡¨ç¤º
            print(f"\nðŸ“‹ {ch_path} ã®å…¨ç•ªçµ„:")
            for file_info in files[:20]:  # æœ€åˆã®20ä»¶ã®ã¿è¡¨ç¤º
                if file_info.get('name', '').endswith('.json'):
                    try:
                        file_path = f"{ch_path}/{file_info['name']}"
                        response = supabase.storage.from_("json-backups").download(file_path)
                        data = json.loads(response.decode('utf-8'))
                        print(f"  - {data.get('program_title', 'ä¸æ˜Ž')}")
                    except:
                        pass
        
    except Exception as e:
        print(f"âŒ æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")

def retry_scraping(event_id, program_title):
    """æŒ‡å®šã•ã‚ŒãŸevent_idã§å†ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’è©¦è¡Œ"""
    print(f"ðŸ”— å†ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°: {event_id}")
    
    # è¤‡æ•°ã®URLãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è©¦è¡Œ
    url_patterns = [
        f"https://bangumi.org/tv_events/seasons?season_id={event_id}",
        f"https://bangumi.org/tv_events/seasons?season_id={event_id}&from=x",
        f"https://bangumi.org/tv_events/seasons?season_id={event_id}&from=fb",
        f"https://bangumi.org/tv_events/{event_id}",
        f"https://bangumi.org/tv_events/seasons/{event_id}"
    ]
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    
    for i, url in enumerate(url_patterns):
        try:
            print(f"  ðŸ”— è©¦è¡Œ {i+1}: {url}")
            response = requests.get(url, headers=headers, timeout=10)
            
            if response.status_code == 200:
                print(f"  âœ… ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: {response.status_code}")
                print(f"  ðŸ“„ ãƒšãƒ¼ã‚¸ã‚µã‚¤ã‚º: {len(response.text)}æ–‡å­—")
                
                # HTMLã‚’è§£æžã—ã¦å‡ºæ¼”è€…ã‚’æŠ½å‡º
                soup = BeautifulSoup(response.text, 'html.parser')
                performers = extract_performers(soup)
                
                if performers:
                    print(f"  ðŸ‘¥ å‡ºæ¼”è€…æ¤œå‡º: {len(performers)}å")
                    for j, performer in enumerate(performers[:5]):  # æœ€åˆã®5åã®ã¿è¡¨ç¤º
                        print(f"    {j+1}. {performer.get('name', 'ä¸æ˜Ž')} (å½¹: {performer.get('role', 'ä¸æ˜Ž')})")
                    if len(performers) > 5:
                        print(f"    ... ä»–{len(performers) - 5}å")
                else:
                    print("  âš ï¸ å‡ºæ¼”è€…æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                
                # HTMLã‚’ä¿å­˜ã—ã¦ãƒ‡ãƒãƒƒã‚°
                debug_filename = f"debug_sunday_discussion_{event_id}_{i+1}.html"
                with open(debug_filename, 'w', encoding='utf-8') as f:
                    f.write(response.text)
                print(f"  ðŸ’¾ HTMLã‚’ä¿å­˜: {debug_filename}")
                
                break
                
            else:
                print(f"  âŒ ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: {response.status_code}")
                
        except Exception as e:
            print(f"  âŒ ã‚¨ãƒ©ãƒ¼: {e}")

def extract_performers(soup):
    """HTMLã‹ã‚‰å‡ºæ¼”è€…æƒ…å ±ã‚’æŠ½å‡º"""
    performers = []
    
    # è¤‡æ•°ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã§å‡ºæ¼”è€…ã‚’æ¤œç´¢
    patterns = [
        # ul.addition ãƒ‘ã‚¿ãƒ¼ãƒ³
        'ul.addition li',
        # ul.talent_panel ãƒ‘ã‚¿ãƒ¼ãƒ³
        'ul.talent_panel li',
        # å‡ºæ¼”è€…ãƒ†ã‚­ã‚¹ãƒˆã‚’å«ã‚€è¦ç´ 
        'div:contains("å‡ºæ¼”è€…")',
        'p:contains("å‡ºæ¼”è€…")',
        'span:contains("å‡ºæ¼”è€…")'
    ]
    
    for pattern in patterns:
        elements = soup.select(pattern)
        if elements:
            for element in elements:
                text = element.get_text(strip=True)
                if text and len(text) > 1:
                    # å‡ºæ¼”è€…åã‚’æŠ½å‡ºï¼ˆç°¡å˜ãªãƒ‘ã‚¿ãƒ¼ãƒ³ãƒžãƒƒãƒãƒ³ã‚°ï¼‰
                    if 'å‡ºæ¼”è€…' in text or 'ã€' in text or 'ã€‘' in text:
                        # ãƒ†ã‚­ã‚¹ãƒˆã‚’è§£æžã—ã¦å‡ºæ¼”è€…åã‚’æŠ½å‡º
                        extracted = parse_performer_text(text)
                        performers.extend(extracted)
    
    # é‡è¤‡ã‚’é™¤åŽ»
    unique_performers = []
    seen_names = set()
    for performer in performers:
        name = performer.get('name', '')
        if name and name not in seen_names:
            unique_performers.append(performer)
            seen_names.add(name)
    
    return unique_performers

def parse_performer_text(text):
    """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰å‡ºæ¼”è€…æƒ…å ±ã‚’è§£æž"""
    performers = []
    
    # å‡ºæ¼”è€…ãƒ†ã‚­ã‚¹ãƒˆã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œç´¢
    if 'å‡ºæ¼”è€…' in text:
        # å‡ºæ¼”è€…ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’æŠ½å‡º
        performer_section = text.split('å‡ºæ¼”è€…')[1] if 'å‡ºæ¼”è€…' in text else text
        
        # ã€ã€‘ã§å›²ã¾ã‚ŒãŸå½¹å‰²ã¨åå‰ã‚’æŠ½å‡º
        import re
        role_pattern = r'ã€([^ã€‘]+)ã€‘([^ã€]+)'
        matches = re.findall(role_pattern, performer_section)
        
        for role, names in matches:
            # åå‰ã‚’åˆ†å‰²ï¼ˆã‚«ãƒ³ãƒžã€ã‚¹ãƒšãƒ¼ã‚¹ãªã©ã§åŒºåˆ‡ã‚‰ã‚Œã¦ã„ã‚‹å ´åˆï¼‰
            name_list = re.split(r'[ã€\s]+', names.strip())
            for name in name_list:
                if name and len(name) > 1:
                    performers.append({
                        'name': name.strip(),
                        'role': role.strip()
                    })
    
    return performers

if __name__ == '__main__':
    find_sunday_discussion() 