import os
import time
import random
import requests
import json
import shutil
from datetime import datetime, timedelta
from bs4 import BeautifulSoup
from supabase import create_client, Client


# é€£æºã‚µãƒ¼ãƒ“ã‚¹ã®è¨­å®š
SUPABASE_URL = os.environ.get("SUPABASE_URL")
SUPABASE_KEY = os.environ.get("SUPABASE_KEY")
DISCORD_WEBHOOK_URL = os.environ.get("DISCORD_WEBHOOK_URL")

# ã€æœ¬æ ¼é‹ç”¨ã€‘å…¨å¯¾è±¡ãƒãƒ£ãƒ³ãƒãƒ«ï¼ˆåœ°ä¸Šæ³¢7å±€ + BS7å±€ï¼‰
TARGET_CHANNELS = [
    # åœ°ä¸Šæ³¢ï¼ˆ7å±€ï¼‰
    "NHKG-TKY", "NHKE-TKY", "NTV-TKY", "TV-ASAHI-TKY", "TBS-TKY", "TV-TOKYO-TKY", "FUJI-TV-TKY",
    # BSãƒãƒ£ãƒ³ãƒãƒ«ï¼ˆ7å±€ï¼‰
    "NHK-BS", "BS-NTV", "BS-ASAHI", "BS-TBS", "BS-TV-TOKYO", "BS-FUJI", "BS11"
]

TARGET_DAYS = 2  # å–å¾—æ—¥æ•°
ROTATION_DAYS = 120  # ãƒ‡ãƒ¼ã‚¿ã®ä¿æŒæ—¥æ•°

# æ”¹è‰¯ã•ã‚ŒãŸãƒãƒ£ãƒ³ãƒãƒ«ãƒãƒƒãƒ”ãƒ³ã‚°ï¼ˆå®Œå…¨ä¸€è‡´å„ªå…ˆï¼‰
CHANNEL_MAPPING = {
    # æ±äº¬ åœ°ä¸Šæ³¢
    "NHKG-TKY": ["NHKç·åˆ", "ï¼®ï¼¨ï¼«ç·åˆ", "NHKç·åˆ æ±äº¬"],
    "NHKE-TKY": ["NHKEãƒ†ãƒ¬", "ï¼®ï¼¨ï¼«ï¼¥ãƒ†ãƒ¬", "NHKEãƒ†ãƒ¬ æ±äº¬"], 
    "NTV-TKY": ["æ—¥ãƒ†ãƒ¬", "æ—¥æœ¬ãƒ†ãƒ¬ãƒ“"],
    "TV-ASAHI-TKY": ["ãƒ†ãƒ¬ãƒ“æœæ—¥", "ãƒ†ãƒ¬æœ"],
    "TBS-TKY": ["TBS", "ï¼´ï¼¢ï¼³"],
    "TV-TOKYO-TKY": ["ãƒ†ãƒ¬æ±", "ãƒ†ãƒ¬ãƒ“æ±äº¬"],
    "FUJI-TV-TKY": ["ãƒ•ã‚¸ãƒ†ãƒ¬ãƒ“", "ãƒ•ã‚¸"],
    "TOKYO-MX": ["TOKYO MX", "ï¼´ï¼¯ï¼«ï¼¹ï¼¯ã€€ï¼­ï¼¸"],
    
    # é–¢æ± åºƒåŸŸ
    "TVS": ["ãƒ†ãƒ¬ç‰"],
    "CTC": ["ãƒãƒãƒ†ãƒ¬ãƒ“", "ãƒãƒãƒ†ãƒ¬"],
    "TVK": ["tvk"],
    
    # BSç„¡æ–™
    "NHK-BS": ["ï¼®ï¼¨ï¼«ã€€ï¼¢ï¼³", "NHK BS"],
    "BS-NTV": ["BSæ—¥ãƒ†ãƒ¬", "ï¼¢ï¼³æ—¥ãƒ†ãƒ¬"],
    "BS-ASAHI": ["BSæœæ—¥", "ï¼¢ï¼³æœæ—¥"],
    "BS-TBS": ["BS-TBS", "ï¼¢ï¼³ï¼ï¼´ï¼¢ï¼³"],
    "BS-TV-TOKYO": ["ï¼¢ï¼³ãƒ†ãƒ¬æ±", "BSãƒ†ãƒ¬æ±"],
    "BS-FUJI": ["BSãƒ•ã‚¸", "ï¼¢ï¼³ãƒ•ã‚¸"],
    "BS11": ["BS11", "ï¼¢ï¼³ï¼‘ï¼‘"],
    "BS12-TWELLV": ["BS12", "ï¼¢ï¼³ï¼‘ï¼’"],
    "BS-YOSHIMOTO": ["ï¼¢ï¼³ã‚ˆã—ã‚‚ã¨"],
    "OUJ-TV-BS": ["æ”¾é€å¤§å­¦"],
    
    # BSæœ‰æ–™
    "WOWOW-PRIME-BS": ["WOWOWãƒ—ãƒ©ã‚¤ãƒ ", "WOWOWãƒ—"],
    "WOWOW-LIVE-BS": ["WOWOWãƒ©ã‚¤ãƒ–"],
    "WOWOW-CINEMA-BS": ["WOWOWã‚·ãƒãƒ"],
    "WOWOW-PLUS-BS": ["WOWOWãƒ—ãƒ©ã‚¹"],
    "STAR-CH-BS": ["ã‚¹ã‚¿ãƒ¼ï½ƒï½ˆ"],
    "JSPORTS-1-BS": ["J SPORTS 1"],
    "JSPORTS-2-BS": ["J SPORTS 2"],
    "JSPORTS-3-BS": ["J SPORTS 3"],
    "JSPORTS-4-BS": ["J SPORTS 4"],
    "GREEN-CH-BS": ["ã‚°ãƒªãƒ¼ãƒ³ãƒãƒ£ãƒ³ãƒãƒ«"],
    "ANIMAX-BS": ["BSã‚¢ãƒ‹ãƒãƒƒã‚¯ã‚¹"],
    "TSURIVISION-BS": ["BSé‡£ã‚Šãƒ“ã‚¸ãƒ§ãƒ³"],
    "DISNEY-CH-BS": ["ãƒ‡ã‚£ã‚ºãƒ‹ãƒ¼ch"],
    "NIHON-EIGA-BS": ["æ—¥æœ¬æ˜ ç”»å°‚é–€ch"],
    
    # ãã®ä»–
    "JCOM-BS": ["J:COM"]
}

def find_channel_code(channel_name):
    """
    ãƒãƒ£ãƒ³ãƒãƒ«åã‹ã‚‰é©åˆ‡ãªãƒãƒ£ãƒ³ãƒãƒ«ã‚³ãƒ¼ãƒ‰ã‚’ç‰¹å®š
    å®Œå…¨ä¸€è‡´ â†’ éƒ¨åˆ†ä¸€è‡´ â†’ ç•ªå·ä»˜ããƒãƒ£ãƒ³ãƒãƒ«å¯¾å¿œã®é †ã§æ¤œç´¢
    """
    if not channel_name:
        return None
    
    # ãƒãƒ£ãƒ³ãƒãƒ«åã®ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
    clean_name = channel_name.strip()
    
    # 1. å®Œå…¨ä¸€è‡´æ¤œç´¢ï¼ˆæœ€å„ªå…ˆï¼‰
    for code, name_list in CHANNEL_MAPPING.items():
        for name in name_list:
            if clean_name == name:
                return code
    
    # 2. ç•ªå·ä»˜ããƒãƒ£ãƒ³ãƒãƒ«åã®å‡¦ç†ï¼ˆä¾‹: "7 ï¼¢ï¼³ãƒ†ãƒ¬æ±"ï¼‰
    # ç•ªå·éƒ¨åˆ†ã‚’é™¤å»ã—ã¦ãƒãƒƒãƒãƒ³ã‚°
    import re
    name_without_number = re.sub(r'^\d+\s*', '', clean_name)
    if name_without_number != clean_name:
        for code, name_list in CHANNEL_MAPPING.items():
            for name in name_list:
                if name_without_number == name:
                    return code
    
    # 3. éƒ¨åˆ†ä¸€è‡´æ¤œç´¢ï¼ˆæœ€ã‚‚å³å¯†ã«ï¼‰
    # BSãƒãƒ£ãƒ³ãƒãƒ«ã‚’å„ªå…ˆçš„ã«ãƒã‚§ãƒƒã‚¯
    bs_matches = []
    terrestrial_matches = []
    
    for code, name_list in CHANNEL_MAPPING.items():
        for name in name_list:
            if name in clean_name or clean_name in name:
                if code.startswith('BS-') or 'BS' in code:
                    bs_matches.append((code, name))
                else:
                    terrestrial_matches.append((code, name))
    
    # BSãƒãƒ£ãƒ³ãƒãƒ«åã«BSãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã€BSãƒãƒƒãƒã‚’å„ªå…ˆ
    if 'BS' in clean_name or 'ï¼¢ï¼³' in clean_name:
        if bs_matches:
            # æœ€ã‚‚é•·ã„ãƒãƒƒãƒã‚’è¿”ã™ï¼ˆã‚ˆã‚Šå…·ä½“çš„ãªãƒãƒƒãƒï¼‰
            best_match = max(bs_matches, key=lambda x: len(x[1]))
            return best_match[0]
    
    # åœ°ä¸Šæ³¢ã®å ´åˆ
    if terrestrial_matches:
        best_match = max(terrestrial_matches, key=lambda x: len(x[1]))
        return best_match[0]
    
    # BSãƒãƒƒãƒãŒã‚ã‚‹å ´åˆã¯ãã‚Œã‚’è¿”ã™
    if bs_matches:
        best_match = max(bs_matches, key=lambda x: len(x[1]))
        return best_match[0]
    
    return None

supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)

def send_discord_notification(message):
    if not DISCORD_WEBHOOK_URL:
        print("âš ï¸ Discord Webhook URLãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        return
    
    print("Discordã¸ã®é€šçŸ¥ã‚’è©¦ã¿ã¾ã™...")
    try:
        response = requests.post(DISCORD_WEBHOOK_URL, json={"content": message}, timeout=15)
        response.raise_for_status()
        print("âœ… Discordã¸ã®é€šçŸ¥ã‚’æ­£å¸¸ã«é€ä¿¡ã—ã¾ã—ãŸã€‚")
    except requests.exceptions.RequestException as e:
        print(f"âŒ Discordé€šçŸ¥ã®é€ä¿¡ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        if e.response is not None:
            print(f" -> Response Status: {e.response.status_code}")
            print(f" -> Response Body: {e.response.text}")

def check_existing_tables():
    """æ—¢å­˜ãƒ†ãƒ¼ãƒ–ãƒ«ã®ç¢ºèªã¨æ§‹é€ æŠŠæ¡"""
    print("\n--- ã‚·ã‚¹ãƒ†ãƒ ç¢ºèª ---")
    
    # æƒ³å®šã•ã‚Œã‚‹ãƒ†ãƒ¼ãƒ–ãƒ«åã®ãƒ‘ã‚¿ãƒ¼ãƒ³
    required_tables = ['programs_epg', 'programs', 'talents']
    appearances_candidates = ['program_talent_appearances', 'appearances']
    
    existing_tables = {}
    appearances_table = None
    
    # å¿…é ˆãƒ†ãƒ¼ãƒ–ãƒ«ã®ç¢ºèª
    for table_name in required_tables:
        try:
            result = supabase.table(table_name).select("*").limit(1).execute()
            existing_tables[table_name] = "âœ…"
        except Exception as e:
            existing_tables[table_name] = "âŒ"
            print(f"âš ï¸ å¿…é ˆãƒ†ãƒ¼ãƒ–ãƒ« {table_name} ã§ã‚¨ãƒ©ãƒ¼: {e}")
    
    # å‡ºæ¼”æƒ…å ±ãƒ†ãƒ¼ãƒ–ãƒ«ã®ç‰¹å®š
    for table_name in appearances_candidates:
        try:
            result = supabase.table(table_name).select("*").limit(1).execute()
            appearances_table = table_name
            existing_tables[table_name] = "âœ…"
            break
        except Exception:
            existing_tables[table_name] = "âŒ"
    
    print(f"ğŸ“‹ ãƒ†ãƒ¼ãƒ–ãƒ«çŠ¶æ³: {existing_tables}")
    if appearances_table:
        print(f"ğŸ¯ å‡ºæ¼”æƒ…å ±ãƒ†ãƒ¼ãƒ–ãƒ«: {appearances_table}")
    else:
        print("âŒ å‡ºæ¼”æƒ…å ±ãƒ†ãƒ¼ãƒ–ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
    
    return appearances_table

def safe_upsert_appearances(appearances_data, table_name, batch_size=500):
    """å®‰å…¨ãªå‡ºæ¼”æƒ…å ±ç™»éŒ²ï¼ˆON CONFLICTåˆ¶ç´„å¯¾å¿œï¼‰"""
    if not appearances_data or not table_name:
        print("ğŸ“ å‡ºæ¼”æƒ…å ±ã®ç™»éŒ²ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
        return 0, 0
    
    success_count = 0
    error_count = 0
    
    print(f"ğŸ“ å‡ºæ¼”æƒ…å ±ç™»éŒ²é–‹å§‹: {len(appearances_data)}ä»¶ â†’ {table_name}")
    
    for i in range(0, len(appearances_data), batch_size):
        batch = appearances_data[i:i + batch_size]
        try:
            # æ–¹æ³•1: INSERTå°‚ç”¨ï¼ˆæ–°è¦ãƒ‡ãƒ¼ã‚¿ã®ã¿ï¼‰
            result = supabase.table(table_name).insert(batch).execute()
            success_count += len(batch)
            print(f"  -> å‡ºæ¼”ãƒãƒƒãƒ {i//batch_size + 1}: {len(batch)}ä»¶ç™»éŒ²å®Œäº†")
            
        except Exception as e:
            # é‡è¤‡ã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯å€‹åˆ¥å‡¦ç†
            if "already exists" in str(e) or "duplicate key" in str(e):
                individual_success = 0
                for single_record in batch:
                    try:
                        supabase.table(table_name).insert([single_record]).execute()
                        individual_success += 1
                    except Exception:
                        # é‡è¤‡ã¯æ­£å¸¸ï¼ˆæ—¢å­˜ãƒ‡ãƒ¼ã‚¿ä¿è­·ï¼‰
                        individual_success += 1
                
                success_count += individual_success
                print(f"  -> å‡ºæ¼”ãƒãƒƒãƒ {i//batch_size + 1}: {individual_success}ä»¶å‡¦ç†å®Œäº†ï¼ˆé‡è¤‡ã‚¹ã‚­ãƒƒãƒ—å«ã‚€ï¼‰")
            else:
                error_count += len(batch)
                print(f"  -> å‡ºæ¼”ãƒãƒƒãƒ {i//batch_size + 1} ç™»éŒ²ã‚¨ãƒ©ãƒ¼: {e}")
    
    return success_count, error_count

def validate_json_data(data):
    """JSONãƒ‡ãƒ¼ã‚¿ã®å¦¥å½“æ€§ã‚’æ¤œè¨¼"""
    try:
        # å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®å­˜åœ¨ç¢ºèª
        required_fields = ['event_id', 'broadcast_date', 'channel', 'program_title']
        for field in required_fields:
            if field not in data or data[field] is None or data[field] == "":
                return False, f"å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ '{field}' ãŒä¸æ­£ã§ã™"
        
        # ãƒ‡ãƒ¼ã‚¿å‹ã®ç¢ºèª
        if not isinstance(data.get('performers', []), list):
            return False, "performersãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãŒãƒªã‚¹ãƒˆå‹ã§ã¯ã‚ã‚Šã¾ã›ã‚“"
        
        # JSONæ–‡å­—åˆ—ã¨ã—ã¦æ­£ã—ãã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚ºã§ãã‚‹ã‹ãƒ†ã‚¹ãƒˆ
        json_test = json.dumps(data, ensure_ascii=False)
        json.loads(json_test)  # ãƒ‡ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚ºãƒ†ã‚¹ãƒˆ
        
        return True, "OK"
    except Exception as e:
        return False, f"JSONæ¤œè¨¼ã‚¨ãƒ©ãƒ¼: {e}"

def safe_json_upload(storage_path, data_dict, max_retries=3):
    """å®‰å…¨ãªJSONãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼ˆãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ä»˜ãï¼‰"""
    
    # 1. ãƒ‡ãƒ¼ã‚¿å¦¥å½“æ€§æ¤œè¨¼
    is_valid, validation_msg = validate_json_data(data_dict)
    if not is_valid:
        print(f"âŒ JSONæ¤œè¨¼å¤±æ•— ({storage_path}): {validation_msg}")
        return False
    
    # 2. JSONæ–‡å­—åˆ—ç”Ÿæˆ
    try:
        json_string = json.dumps(data_dict, ensure_ascii=False, indent=2)
        if len(json_string) < 50:  # æœ€å°ã‚µã‚¤ã‚ºãƒã‚§ãƒƒã‚¯
            print(f"âŒ JSONãƒ‡ãƒ¼ã‚¿ãŒå°ã•ã™ãã¾ã™ ({storage_path}): {len(json_string)}æ–‡å­—")
            return False
    except Exception as e:
        print(f"âŒ JSONç”Ÿæˆã‚¨ãƒ©ãƒ¼ ({storage_path}): {e}")
        return False
    
    # 3. ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰è©¦è¡Œï¼ˆãƒªãƒˆãƒ©ã‚¤ä»˜ãï¼‰
    for attempt in range(max_retries):
        try:
            supabase.storage.from_('json-backups').upload(
                path=storage_path,
                file=json_string.encode('utf-8'),
                file_options={
                    "content-type": "application/json;charset=utf-8", 
                    "upsert": "true"
                }
            )
            return True
        except Exception as e:
            print(f"âš ï¸ JSONä¿å­˜è©¦è¡Œ {attempt + 1}/{max_retries} å¤±æ•— ({storage_path}): {e}")
            if attempt < max_retries - 1:
                time.sleep(2 ** attempt)  # æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•
    
    return False

def clean_text(text):
    """ãƒ†ã‚­ã‚¹ãƒˆã®ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆNoneã‚„ç©ºæ–‡å­—ã®å‡¦ç†ï¼‰"""
    if text is None:
        return ""
    return str(text).strip()

def safe_extract_talent_info(link_element):
    """å®‰å…¨ãªã‚¿ãƒ¬ãƒ³ãƒˆæƒ…å ±æŠ½å‡º"""
    try:
        name = clean_text(link_element.get_text(strip=True))
        href = link_element.get("href", "")
        
        # åŸºæœ¬çš„ãªå¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯
        if not name or not href:
            return None
        
        if href.rstrip("/").endswith("talents"):
            return None
            
        # talent_idã®æŠ½å‡ºã¨æ¤œè¨¼
        talent_id = href.rstrip("/").split("/")[-1].split("?")[0]
        if not talent_id.isdigit():
            return None
            
        full_link = "https://bangumi.org" + href if href.startswith("/") else href
        
        return {
            "talent_id": talent_id,
            "name": name,
            "link": full_link
        }
    except Exception as e:
        print(f"âš ï¸ ã‚¿ãƒ¬ãƒ³ãƒˆæƒ…å ±æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
        return None

def extract_performers_from_html(soup_detail):
    """HTMLã‹ã‚‰å‡ºæ¼”è€…æƒ…å ±ã‚’æŠ½å‡ºï¼ˆæ”¹è‰¯ç‰ˆï¼‰"""
    performers = {}
    
    try:
        # æ–¹æ³•1: ul.additionå†…ã®å‡ºæ¼”è€…æƒ…å ±ã‚’å–å¾—
        addition_section = soup_detail.find("ul", class_="addition")
        if addition_section:
            performer_text = addition_section.get_text(strip=True)
            print(f"  ğŸ” å‡ºæ¼”è€…ãƒ†ã‚­ã‚¹ãƒˆæ¤œå‡º: {performer_text[:100]}...")
            
            # å‡ºæ¼”è€…ãƒªãƒ³ã‚¯ã‚’æŠ½å‡º
            performer_links = addition_section.find_all("a", href=lambda x: x and "/talents/" in x)
            for link in performer_links:
                talent_info = safe_extract_talent_info(link)
                if talent_info:
                    performers[talent_info["name"]] = talent_info["link"]
        
        # æ–¹æ³•2: ul.talent_panelå†…ã®ã‚¿ãƒ¬ãƒ³ãƒˆæƒ…å ±ã‚’å–å¾—
        talent_panel = soup_detail.find("ul", class_="talent_panel")
        if talent_panel:
            talent_links = talent_panel.find_all("a", href=lambda x: x and "/talents/" in x)
            for link in talent_links:
                talent_info = safe_extract_talent_info(link)
                if talent_info:
                    performers[talent_info["name"]] = talent_info["link"]
        
        # æ–¹æ³•3: ãƒšãƒ¼ã‚¸å…¨ä½“ã‹ã‚‰ã‚¿ãƒ¬ãƒ³ãƒˆãƒªãƒ³ã‚¯ã‚’å–å¾—ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
        if not performers:
            all_talent_links = soup_detail.find_all("a", href=lambda x: x and "/talents/" in x)
            for link in all_talent_links:
                talent_info = safe_extract_talent_info(link)
                if talent_info:
                    performers[talent_info["name"]] = talent_info["link"]
        
        # æ–¹æ³•4: description_detailã‹ã‚‰å‡ºæ¼”è€…æƒ…å ±ã‚’æŠ½å‡ºï¼ˆè£œå®Œï¼‰
        description_detail = soup_detail.find('meta', {'name': 'description'})
        if description_detail:
            detail_text = description_detail.get('content', '')
            if detail_text and 'ã€å‡ºæ¼”ã€‘' in detail_text:
                extracted_performers = extract_performers_from_description(detail_text)
                for performer in extracted_performers:
                    name = performer.get('name', '')
                    if name and name not in performers:
                        # talent_idãŒãªã„å ´åˆã¯ä»®ã®IDã‚’ç”Ÿæˆ
                        performers[name] = f"extracted_{hash(name) % 1000000}"
        
        print(f"  ğŸ‘¥ å‡ºæ¼”è€…æ¤œå‡º: {len(performers)}å")
        if performers:
            for name in list(performers.keys())[:3]:  # æœ€åˆã®3åã‚’è¡¨ç¤º
                print(f"    - {name}")
            if len(performers) > 3:
                print(f"    ... ä»–{len(performers) - 3}å")
        
    except Exception as e:
        print(f"âš ï¸ å‡ºæ¼”è€…æƒ…å ±æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
    
    return performers

def extract_performers_from_description(description_text):
    """description_detailã‹ã‚‰å‡ºæ¼”è€…æƒ…å ±ã‚’æŠ½å‡º"""
    performers = []
    
    try:
        # ã€å‡ºæ¼”ã€‘ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’æŠ½å‡º
        if 'ã€å‡ºæ¼”ã€‘' in description_text:
            start = description_text.find('ã€å‡ºæ¼”ã€‘') + len('ã€å‡ºæ¼”ã€‘')
            end = description_text.find('ã€', start)
            if end == -1:
                end = len(description_text)
            performer_section = description_text[start:end].strip()
            
            # å½¹è·ãƒ»åå‰ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æŠ½å‡º
            import re
            pattern = r'([^ãƒ»]+)ãƒ»([^ã€]+)'
            matches = re.findall(pattern, performer_section)
            
            for role, name in matches:
                performers.append({
                    'name': name.strip(),
                    'role': role.strip()
                })
            
            print(f"  ğŸ“ description_detailã‹ã‚‰{len(performers)}åã®å‡ºæ¼”è€…ã‚’æŠ½å‡º")
    
    except Exception as e:
        print(f"âš ï¸ description_detailã‹ã‚‰ã®å‡ºæ¼”è€…æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
    
    return performers

def archive_old_db_records():
    print("\n--- å¤ã„ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ¬ã‚³ãƒ¼ãƒ‰ã®ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–é–‹å§‹ ---")
    cutoff_date_str = (datetime.now() - timedelta(days=ROTATION_DAYS)).strftime('%Y-%m-%d')
    print(f"{cutoff_date_str} ã‚ˆã‚Šå‰ã®ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã—ã¾ã™ã€‚")
    try:
        for table_name in ["programs_epg", "programs"]:
            response = supabase.table(table_name).select("*").lt('broadcast_date', cutoff_date_str).execute()
            if response.data:
                print(f" -> {table_name}: {len(response.data)}ä»¶ã‚’ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ä¸­...")
                try:
                    supabase.table(f"{table_name}_archive").upsert(response.data).execute()
                    supabase.table(table_name).delete().lt('broadcast_date', cutoff_date_str).execute()
                except Exception as archive_error:
                    print(f"âš ï¸ {table_name}ã®ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã‚’ã‚¹ã‚­ãƒƒãƒ—: {archive_error}")
        print("âœ… å¤ã„DBãƒ¬ã‚³ãƒ¼ãƒ‰ã®ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–å®Œäº†ã€‚")
    except Exception as e:
        print(f"âŒ DBãƒ¬ã‚³ãƒ¼ãƒ‰ã®ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")

def main():
    print("ğŸš€ ã€æœ¬æ ¼é‹ç”¨ã€‘ç•ªçµ„è¡¨ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’é–‹å§‹ã—ã¾ã™ã€‚")
    print(f"ğŸ“‹ å–å¾—å¯¾è±¡: åœ°ä¸Šæ³¢7å±€ + BS7å±€ = è¨ˆ{len(TARGET_CHANNELS)}å±€")
    print(f"ğŸ“… å–å¾—æœŸé–“: {TARGET_DAYS}æ—¥é–“")

    # ã‚·ã‚¹ãƒ†ãƒ ç¢ºèª
    appearances_table_name = check_existing_tables()
    if not appearances_table_name:
        print("âš ï¸ å‡ºæ¼”æƒ…å ±ãƒ†ãƒ¼ãƒ–ãƒ«ãŒè¦‹ã¤ã‹ã‚‰ãªã„ãŸã‚ã€å‡ºæ¼”æƒ…å ±ã®ç™»éŒ²ã¯è¡Œã„ã¾ã›ã‚“ã€‚")

    # --- 1. EPGåŸºæœ¬æƒ…å ±ã®å–å¾— ---
    epg_data_to_upsert = []
    processed_event_ids = set()
    target_dates = [(datetime.now() + timedelta(days=i)) for i in range(-1, TARGET_DAYS + 1)]

    print("\n--- EPGåŸºæœ¬æƒ…å ±ã®å–å¾—é–‹å§‹ ---")
    bs_channel_count = 0
    
    for ch_type in ["td", "bs"]:
        for target_date in target_dates:
            date_str_url = target_date.strftime("%Y%m%d")
            date_str_db = target_date.strftime("%Y-%m-%d")
            url = f"https://bangumi.org/epg/{ch_type}?broad_cast_date={date_str_url}"
            if ch_type == "td":
                url += "&ggm_group_id=42"

            print(f"ã‚¢ã‚¯ã‚»ã‚¹ä¸­: {url}")
            try:
                res = requests.get(url, timeout=20)
                res.raise_for_status()
                soup = BeautifulSoup(res.text, 'html.parser')
                channel_tags = soup.find_all("li", class_="js_channel topmost")
                channel_names = [tag.text.strip() for tag in channel_tags]
                program_lines = soup.find_all("ul", id=lambda x: x and x.startswith("program_line_"))

                for i, line in enumerate(program_lines):
                    programs = line.find_all("li")
                    for program_tag in programs:
                        a_tag = program_tag.find("a", class_="title_link")
                        if not a_tag:
                            continue

                        href = a_tag.get("href", "")
                        event_id = href.split("/")[-1].split("?")[0]
                        if not event_id or event_id in processed_event_ids:
                            continue

                        channel_name = channel_names[i] if i < len(channel_names) else "ä¸æ˜"
                        # æ”¹è‰¯ã•ã‚ŒãŸãƒãƒ£ãƒ³ãƒãƒ«ã‚³ãƒ¼ãƒ‰ç‰¹å®š
                        channel_code = find_channel_code(channel_name)

                        # BSãƒãƒ£ãƒ³ãƒãƒ«ã®ãƒãƒƒãƒ”ãƒ³ã‚°çŠ¶æ³ã‚’ãƒ­ã‚°å‡ºåŠ›
                        if ('BS' in channel_name or 'ï¼¢ï¼³' in channel_name) and channel_code in TARGET_CHANNELS:
                            bs_channel_count += 1
                            if bs_channel_count <= 5:  # æœ€åˆã®5ä»¶ã®ã¿å‡ºåŠ›
                                print(f"  ğŸ” BSãƒãƒ£ãƒ³ãƒãƒ«æ¤œå‡º: '{channel_name}' â†’ '{channel_code}'")

                        # ç•ªçµ„ã‚¿ã‚¤ãƒˆãƒ«ã¨è©³ç´°ã®å®‰å…¨ãªå–å¾—
                        title_elem = a_tag.find("p", class_="program_title")
                        detail_elem = a_tag.find("p", class_="program_detail")
                        
                        program_title = clean_text(title_elem.text if title_elem else "")
                        program_detail = clean_text(detail_elem.text if detail_elem else "")

                        epg_data_to_upsert.append({
                            "event_id": event_id,
                            "broadcast_date": date_str_db,
                            "channel": channel_name,
                            "start_time": clean_text(program_tag.get("s", "")),
                            "end_time": clean_text(program_tag.get("e", "")),
                            "program_title": program_title,
                            "program_detail": program_detail,
                            "link": "https://bangumi.org" + href,
                            "region": "æ±äº¬",
                            "channel_code": channel_code
                        })
                        processed_event_ids.add(event_id)
            except Exception as e:
                print(f"  -> EPGãƒšãƒ¼ã‚¸å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
                continue

    if not epg_data_to_upsert:
        raise Exception("EPGæƒ…å ±ãŒä¸€ä»¶ã‚‚å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚å‡¦ç†ã‚’ä¸­æ–­ã—ã¾ã™ã€‚")

    print(f"\nâœ… {len(epg_data_to_upsert)}ä»¶ã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªEPGæƒ…å ±ã‚’å–å¾—ã€‚DBã«ç™»éŒ²ã—ã¾ã™...")
    
    # EPGãƒ‡ãƒ¼ã‚¿ã‚’ãƒãƒƒãƒå‡¦ç†ã§ç™»éŒ²
    batch_size = 1000
    for i in range(0, len(epg_data_to_upsert), batch_size):
        batch = epg_data_to_upsert[i:i + batch_size]
        try:
            supabase.table('programs_epg').upsert(batch, on_conflict='event_id').execute()
            print(f"  -> EPGãƒãƒƒãƒ {i//batch_size + 1}: {len(batch)}ä»¶ç™»éŒ²å®Œäº†")
        except Exception as e:
            print(f"  -> EPGãƒãƒƒãƒ {i//batch_size + 1} ç™»éŒ²ã‚¨ãƒ©ãƒ¼: {e}")

    # --- 2. ç•ªçµ„è©³ç´°æƒ…å ±ã®å–å¾— ---
    print("\n--- ç•ªçµ„è©³ç´°æƒ…å ±ã®å–å¾—é–‹å§‹ ---")
    program_details_to_upsert = []
    appearances_to_upsert = []
    talents_seen = {}
    json_upload_success = 0
    json_upload_errors = 0

    # å–å¾—å¯¾è±¡ã®ç•ªçµ„ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    target_programs = [p for p in epg_data_to_upsert if p.get('channel_code') in TARGET_CHANNELS]
    print(f"ğŸ“º è©³ç´°å–å¾—å¯¾è±¡: {len(target_programs)}ç•ªçµ„ï¼ˆå…¨{len(epg_data_to_upsert)}ç•ªçµ„ä¸­ï¼‰")

    for program in target_programs:
        if not program.get('link'):
            continue

        print(f"è©³ç´°å–å¾—ä¸­: {program['program_title']}")
        try:
            # ã‚ˆã‚Šé•·ã„ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã§ãƒšãƒ¼ã‚¸ã‚’å–å¾—
            res_detail = requests.get(program['link'], timeout=30)
            res_detail.raise_for_status()
            soup_detail = BeautifulSoup(res_detail.text, 'html.parser')
            
            # ãƒšãƒ¼ã‚¸ã®èª­ã¿è¾¼ã¿çŠ¶æ³ã‚’ç¢ºèª
            print(f"  ğŸ“„ ãƒšãƒ¼ã‚¸ã‚µã‚¤ã‚º: {len(res_detail.text)}æ–‡å­—")

            title = clean_text(program['program_title'])
            
            # ãƒ¡ã‚¿æƒ…å ±ã®å®‰å…¨ãªå–å¾—
            meta_desc = soup_detail.find("meta", {"name": "description"})
            description = clean_text(meta_desc["content"] if meta_desc else "")
            
            letter_body = soup_detail.find("p", class_="letter_body")
            description_detail = clean_text(letter_body.get_text(strip=True) if letter_body else "")
            
            genre_tag = soup_detail.find("p", class_="genre nomal")
            genre = clean_text(genre_tag.get_text(strip=True).replace("\u3000", " ") if genre_tag else "")
            
            site_tag = soup_detail.select_one("ul.related_link a")
            official_website = clean_text(site_tag.get("href") if site_tag else "")

            # å‡ºæ¼”è€…ãƒªãƒ³ã‚¯æŠ½å‡ºï¼ˆå …ç‰¢åŒ–ï¼‰
            performer_links = {}
            # æ”¹è‰¯ã•ã‚ŒãŸå‡ºæ¼”è€…æƒ…å ±æŠ½å‡ºé–¢æ•°ã‚’ä½¿ç”¨
            performer_links = extract_performers_from_html(soup_detail)
            
            # ãƒ‡ãƒãƒƒã‚°æƒ…å ±: HTMLã®æ§‹é€ ç¢ºèª
            if not performer_links:
                print(f"  âš ï¸ å‡ºæ¼”è€…æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚HTMLæ§‹é€ ã‚’ç¢ºèªä¸­...")
                addition_section = soup_detail.find("ul", class_="addition")
                talent_panel = soup_detail.find("ul", class_="talent_panel")
                print(f"    - ul.addition: {'ã‚ã‚Š' if addition_section else 'ãªã—'}")
                print(f"    - ul.talent_panel: {'ã‚ã‚Š' if talent_panel else 'ãªã—'}")
                
                # ãƒšãƒ¼ã‚¸å…¨ä½“ã®ã‚¿ãƒ¬ãƒ³ãƒˆãƒªãƒ³ã‚¯æ•°ã‚’ç¢ºèª
                all_talent_links = soup_detail.find_all("a", href=lambda x: x and "/talents/" in x)
                print(f"    - ãƒšãƒ¼ã‚¸å…¨ä½“ã®ã‚¿ãƒ¬ãƒ³ãƒˆãƒªãƒ³ã‚¯: {len(all_talent_links)}å€‹")
                
                # ãƒ‡ãƒãƒƒã‚°ç”¨ã«HTMLã‚’ä¿å­˜ï¼ˆæœ€åˆã®5ä»¶ã®ã¿ï¼‰
                if len(program_details_to_upsert) < 5:
                    debug_filename = f"debug_{program['event_id']}.html"
                    try:
                        with open(debug_filename, 'w', encoding='utf-8') as f:
                            f.write(res_detail.text)
                        print(f"    - ãƒ‡ãƒãƒƒã‚°HTMLä¿å­˜: {debug_filename}")
                    except Exception as e:
                        print(f"    - ãƒ‡ãƒãƒƒã‚°HTMLä¿å­˜å¤±æ•—: {e}")

            # ã‚¿ãƒ¬ãƒ³ãƒˆæƒ…å ±ã®å‡¦ç†
            talents_to_upsert = []
            current_program_appearances = []
            
            for name, link in performer_links.items():
                try:
                    talent_id = link.rstrip("/").split("/")[-1].split("?")[0]
                    if talent_id.isdigit():
                        # ã‚¿ãƒ¬ãƒ³ãƒˆæƒ…å ±ã®é‡è¤‡ãƒã‚§ãƒƒã‚¯
                        if talent_id not in talents_seen:
                            talents_to_upsert.append({
                                "talent_id": talent_id,
                                "name": name,
                                "link": link
                            })
                            talents_seen[talent_id] = name
                        
                        # å‡ºæ¼”æƒ…å ±
                        current_program_appearances.append({
                            "program_event_id": program['event_id'],
                            "talent_id": talent_id
                        })
                except Exception as e:
                    print(f"âš ï¸ ã‚¿ãƒ¬ãƒ³ãƒˆå‡¦ç†ã‚¨ãƒ©ãƒ¼ ({name}): {e}")
                    continue

            # ã‚¿ãƒ¬ãƒ³ãƒˆæƒ…å ±ã®DBç™»éŒ²
            if talents_to_upsert:
                try:
                    supabase.table('talents').upsert(talents_to_upsert, on_conflict='talent_id').execute()
                except Exception as e:
                    print(f"âš ï¸ ã‚¿ãƒ¬ãƒ³ãƒˆç™»éŒ²ã‚¨ãƒ©ãƒ¼: {e}")

            # å‡ºæ¼”æƒ…å ±ã‚’ã¾ã¨ã‚ã¦è¿½åŠ 
            appearances_to_upsert.extend(current_program_appearances)

            # ç•ªçµ„è©³ç´°ãƒ‡ãƒ¼ã‚¿ã®æ§‹ç¯‰
            db_data = {
                "event_id": program['event_id'],
                "broadcast_date": program['broadcast_date'],
                "channel": program['channel'],
                "start_time": program['start_time'],
                "end_time": program['end_time'],
                "master_title": title.split("ã€€")[0] if "ã€€" in title and title else title,
                "program_title": title,
                "description": description,
                "description_detail": description_detail,
                "genre": genre,
                "official_website": official_website,
                "channel_code": program['channel_code']
            }
            program_details_to_upsert.append(db_data)

            # JSONãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆï¼ˆå¦¥å½“æ€§æ¤œè¨¼ä»˜ãï¼‰
            date_str = program['broadcast_date']
            start_hhmm = program['start_time'][8:12] if len(program['start_time']) >= 12 else "0000"
            file_name = f"{date_str}-{start_hhmm}_{program['channel_code']}_{program['event_id']}.json"
            storage_path = f"{date_str}/{program['channel_code']}/{file_name}"
            
            # JSONç”¨ãƒ‡ãƒ¼ã‚¿ï¼ˆå¿…è¦ãªãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®ã¿å«ã‚€ã€å®‰å…¨ãªã‚³ãƒ”ãƒ¼ä½œæˆï¼‰
            json_data = {
                **db_data,
                "performers": [
                    {
                        "talent_id": talent["talent_id"],
                        "name": talent["name"],
                        "link": talent["link"]
                    } for talent in talents_to_upsert
                ] if talents_to_upsert else [],
                "performer_count": len(talents_to_upsert),
                "created_at": datetime.now().isoformat()
            }

            # JSONä¿å­˜è©¦è¡Œ
            if safe_json_upload(storage_path, json_data):
                print(f"  -> JSONä¿å­˜å®Œäº†: {storage_path}")
                json_upload_success += 1
            else:
                print(f"  -> JSONä¿å­˜å¤±æ•—: {storage_path}")
                json_upload_errors += 1

            time.sleep(random.uniform(1.5, 2.5))
            
        except Exception as e:
            print(f"âŒ ç•ªçµ„è©³ç´°å–å¾—å¤±æ•—: {program['program_title']} - {e}")
            continue

    # --- 3. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¸€æ‹¬ç™»éŒ² ---
    if program_details_to_upsert:
        print(f"\nâœ… {len(program_details_to_upsert)}ä»¶ã®è©³ç´°æƒ…å ±ã‚’DBç™»éŒ²ã—ã¾ã™...")
        batch_size = 500
        for i in range(0, len(program_details_to_upsert), batch_size):
            batch = program_details_to_upsert[i:i + batch_size]
            try:
                supabase.table('programs').upsert(batch, on_conflict='event_id').execute()
                print(f"  -> è©³ç´°ãƒãƒƒãƒ {i//batch_size + 1}: {len(batch)}ä»¶ç™»éŒ²å®Œäº†")
            except Exception as e:
                print(f"  -> è©³ç´°ãƒãƒƒãƒ {i//batch_size + 1} ç™»éŒ²ã‚¨ãƒ©ãƒ¼: {e}")

    # --- 4. å‡ºæ¼”æƒ…å ±ç™»éŒ² ---
    if appearances_to_upsert and appearances_table_name:
        success, errors = safe_upsert_appearances(appearances_to_upsert, appearances_table_name)
        print(f"âœ… å‡ºæ¼”æƒ…å ±ç™»éŒ²çµæœ: æˆåŠŸ {success}ä»¶, å¤±æ•— {errors}ä»¶")
    elif not appearances_table_name:
        print("âš ï¸ å‡ºæ¼”æƒ…å ±ãƒ†ãƒ¼ãƒ–ãƒ«ãŒç‰¹å®šã§ããªã„ãŸã‚ã€å‡ºæ¼”æƒ…å ±ã®ç™»éŒ²ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
    else:
        print("ğŸ“ å‡ºæ¼”æƒ…å ±ãªã—")

    # æœ€çµ‚çµæœã‚µãƒãƒªãƒ¼
    channel_breakdown = {}
    for program in target_programs:
        code = program.get('channel_code')
        if code:
            channel_breakdown[code] = channel_breakdown.get(code, 0) + 1

    print(f"\nğŸ“Š ã€æœ¬æ ¼é‹ç”¨ã€‘æœ€çµ‚çµæœã‚µãƒãƒªãƒ¼:")
    print(f"  â€¢ EPGå–å¾—: {len(epg_data_to_upsert)}ä»¶")
    print(f"  â€¢ è©³ç´°å–å¾—: {len(program_details_to_upsert)}ä»¶")
    print(f"  â€¢ JSONä¿å­˜: æˆåŠŸ {json_upload_success}ä»¶, å¤±æ•— {json_upload_errors}ä»¶")
    print(f"  â€¢ å‡ºæ¼”æƒ…å ±: {len(appearances_to_upsert)}ä»¶")
    print(f"  â€¢ å¯¾è±¡ãƒãƒ£ãƒ³ãƒãƒ«: {len(TARGET_CHANNELS)}å±€")
    
    # ãƒãƒ£ãƒ³ãƒãƒ«åˆ¥å†…è¨³ï¼ˆåœ°ä¸Šæ³¢ã¨BSã‚’åˆ†ã‘ã¦è¡¨ç¤ºï¼‰
    terrestrial_count = sum(count for code, count in channel_breakdown.items() if not code.startswith('BS-') and 'BS' not in code)
    bs_count = sum(count for code, count in channel_breakdown.items() if code.startswith('BS-') or 'BS' in code)
    
    print(f"  â€¢ åœ°ä¸Šæ³¢ç•ªçµ„: {terrestrial_count}ä»¶")
    print(f"  â€¢ BSç•ªçµ„: {bs_count}ä»¶")
    
    print(f"\nğŸ“‹ ãƒãƒ£ãƒ³ãƒãƒ«åˆ¥è©³ç´°:")
    for channel_code, count in sorted(channel_breakdown.items()):
        channel_type = "ğŸ¢" if not channel_code.startswith('BS-') and 'BS' not in channel_code else "ğŸ“¡"
        print(f"    {channel_type} {channel_code}: {count}ä»¶")
    
    print("\nğŸ‰ æœ¬æ ¼é‹ç”¨ãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸã€‚")
    
    return len(epg_data_to_upsert), len(program_details_to_upsert)

        
if __name__ == '__main__':
    start_date = (datetime.now() - timedelta(days=1)).strftime('%Y-%m-%d')
    end_date = (datetime.now() + timedelta(days=TARGET_DAYS)).strftime('%Y-%m-%d')
    
    try:
        epg_count, detail_count = main()
        archive_old_db_records()
        
        # æˆåŠŸé€šçŸ¥ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ï¼ˆè©³ç´°ç‰ˆï¼‰
        success_message = (
            f"âœ… ã€æœ¬æ ¼é‹ç”¨ã€‘ç•ªçµ„è¡¨ã‚¹ã‚¯ãƒªãƒ—ãƒˆãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸã€‚\n\n"
            f"**ğŸ“… å‡¦ç†æœŸé–“**: {start_date} ï½ {end_date}\n"
            f"**ğŸ“Š å–å¾—çµæœ**:\n"
            f"  â€¢ ç•ªçµ„æ¦‚è¦: {epg_count}ä»¶\n"
            f"  â€¢ ç•ªçµ„è©³ç´°: {detail_count}ä»¶\n"
            f"**ğŸ“º å¯¾è±¡ãƒãƒ£ãƒ³ãƒãƒ«**: åœ°ä¸Šæ³¢7å±€ + BS7å±€\n"
            f"**ğŸ”§ ä¿®æ­£å†…å®¹**:\n"
            f"  â€¢ ãƒãƒ£ãƒ³ãƒãƒ«ãƒãƒƒãƒ”ãƒ³ã‚°å•é¡Œè§£æ±º\n"
            f"  â€¢ JSONä¿å­˜ã‚¨ãƒ©ãƒ¼è§£æ±º\n"
            f"  â€¢ æ—¢å­˜ãƒ†ãƒ¼ãƒ–ãƒ«æ§‹é€ å¯¾å¿œ\n"
            f"**ğŸš€ ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹**: æœ¬æ ¼é‹ç”¨é–‹å§‹"
        )
        send_discord_notification(success_message)
        
    except Exception as e:
        error_message = (
            f"ğŸš¨ ã€æœ¬æ ¼é‹ç”¨ã€‘ç•ªçµ„è¡¨ã‚¹ã‚¯ãƒªãƒ—ãƒˆã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚\n\n"
            f"**ã‚¨ãƒ©ãƒ¼å†…å®¹**:\n```\n{e}\n```\n\n"
            f"**å¯¾è±¡æœŸé–“**: {start_date} ï½ {end_date}\n"
            f"**å¯¾è±¡**: åœ°ä¸Šæ³¢7å±€ + BS7å±€"
        )
        print(error_message)
        send_discord_notification(error_message)
