import os
import time
import random
import requests
import json
import shutil
from datetime import datetime, timedelta
from bs4 import BeautifulSoup
from supabase import create_client, Client


# é€£æºã‚µãƒ¼ãƒ“ã‚¹ã®è¨­å®š
SUPABASE_URL = os.environ.get("SUPABASE_URL")
SUPABASE_KEY = os.environ.get("SUPABASE_KEY")
DISCORD_WEBHOOK_URL = os.environ.get("DISCORD_WEBHOOK_URL")

# ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å¯¾è±¡
TARGET_CHANNELS = ["NHKG-TKY", "NHKE-TKY", "NTV-TKY", "TV-ASAHI-TKY", "TBS-TKY", "TV-TOKYO-TKY", "FUJI-TV-TKY"]
TARGET_DAYS = 2 # å–å¾—æ—¥æ•°
ROTATION_DAYS = 120 # ãƒ‡ãƒ¼ã‚¿ã®ä¿æŒæ—¥æ•°

CHANNEL_MAPPING = {
    # æ±äº¬ åœ°ä¸Šæ³¢
    "NHKG-TKY": "NHKç·åˆ", "NHKE-TKY": "NHKEãƒ†ãƒ¬", "NTV-TKY": "æ—¥ãƒ†ãƒ¬",
    "TV-ASAHI-TKY": "ãƒ†ãƒ¬ãƒ“æœæ—¥", "TBS-TKY": "TBS", "TV-TOKYO-TKY": "ãƒ†ãƒ¬æ±",
    "FUJI-TV-TKY": "ãƒ•ã‚¸ãƒ†ãƒ¬ãƒ“", "TOKYO-MX": "TOKYO",
    # é–¢æ± åºƒåŸŸ
    "TVS": "ãƒ†ãƒ¬ç‰", "CTC": "ãƒãƒãƒ†ãƒ¬", "TVK": "tvk",
    # BSç„¡æ–™
    "NHK-BS": "ï¼®ï¼¨ï¼«ã€€ï¼¢ï¼³", "BS-NTV": "BSæ—¥ãƒ†ãƒ¬", "BS-ASAHI": "BSæœæ—¥",
    "BS-TBS": "BS-TBS", "BS-TV-TOKYO": "ï¼¢ï¼³ãƒ†ãƒ¬æ±", "BS-FUJI": "BSãƒ•ã‚¸",
    "BS11": "BS11", "BS12-TWELLV": "BS12", "BS-YOSHIMOTO": "ï¼¢ï¼³ã‚ˆã—ã‚‚ã¨",
    "OUJ-TV-BS": "æ”¾é€å¤§å­¦",
    # BSæœ‰æ–™
    "WOWOW-PRIME-BS": "WOWOWãƒ—", "WOWOW-LIVE-BS": "WOWOWãƒ©ã‚¤ãƒ–",
    "WOWOW-CINEMA-BS": "WOWOWã‚·ãƒãƒ", "WOWOW-PLUS-BS": "WOWOWãƒ—ãƒ©ã‚¹",
    "STAR-CH-BS": "ã‚¹ã‚¿ãƒ¼ï½ƒï½ˆ",
    "JSPORTS-1-BS": "J SPORTS 1", "JSPORTS-2-BS": "J SPORTS 2",
    "JSPORTS-3-BS": "J SPORTS 3", "JSPORTS-4-BS": "J SPORTS 4",
    "GREEN-CH-BS": "ã‚°ãƒªãƒ¼ãƒ³ãƒãƒ£ãƒ³ãƒãƒ«", "ANIMAX-BS": "BSã‚¢ãƒ‹ãƒãƒƒã‚¯ã‚¹",
    "TSURIVISION-BS": "BSé‡£ã‚Šãƒ“ã‚¸ãƒ§ãƒ³", "DISNEY-CH-BS": "ãƒ‡ã‚£ã‚ºãƒ‹ãƒ¼ch",
    "NIHON-EIGA-BS": "æ—¥æœ¬æ˜ ç”»å°‚é–€ch",
    # ãã®ä»–
    "JCOM-BS": "J:COM"
}

supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)

def send_discord_notification(message):
    if not DISCORD_WEBHOOK_URL:
        print("âš ï¸ Discord Webhook URLãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        return
    
    print("Discordã¸ã®é€šçŸ¥ã‚’è©¦ã¿ã¾ã™...")
    try:
        response = requests.post(DISCORD_WEBHOOK_URL, json={"content": message}, timeout=15)
        # HTTPã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰ãŒ2xxï¼ˆæˆåŠŸï¼‰ã§ãªã„å ´åˆã«ä¾‹å¤–ã‚’ç™ºç”Ÿã•ã›ã‚‹
        response.raise_for_status()
        print("âœ… Discordã¸ã®é€šçŸ¥ã‚’æ­£å¸¸ã«é€ä¿¡ã—ã¾ã—ãŸã€‚")
    except requests.exceptions.RequestException as e:
        # é€šä¿¡ã‚¨ãƒ©ãƒ¼ã‚„HTTPã‚¨ãƒ©ãƒ¼ã‚’ã‚­ãƒ£ãƒƒãƒã—ã¦è©³ç´°ã‚’å‡ºåŠ›
        print(f"âŒ Discordé€šçŸ¥ã®é€ä¿¡ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        if e.response is not None:
            print(f" -> Response Status: {e.response.status_code}")
            print(f" -> Response Body: {e.response.text}")

def archive_old_db_records():
    print("\n--- å¤ã„ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ¬ã‚³ãƒ¼ãƒ‰ã®ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–é–‹å§‹ ---")
    cutoff_date_str = (datetime.now() - timedelta(days=ROTATION_DAYS)).strftime('%Y-%m-%d')
    print(f"{cutoff_date_str} ã‚ˆã‚Šå‰ã®ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã—ã¾ã™ã€‚")
    try:
        for table_name in ["programs_epg", "programs"]:
            response = supabase.table(table_name).select("*").lt('broadcast_date', cutoff_date_str).execute()
            if response.data:
                print(f" -> {table_name}: {len(response.data)}ä»¶ã‚’ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ä¸­...")
                supabase.table(f"{table_name}_archive").upsert(response.data).execute()
                supabase.table(table_name).delete().lt('broadcast_date', cutoff_date_str).execute()
        print("âœ… å¤ã„DBãƒ¬ã‚³ãƒ¼ãƒ‰ã®ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–å®Œäº†ã€‚")
    except Exception as e:
        print(f"âŒ DBãƒ¬ã‚³ãƒ¼ãƒ‰ã®ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")

def archive_old_files():
    print("\n--- å¤ã„JSONãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–é–‹å§‹ ---")
    cutoff_date = datetime.now() - timedelta(days=ROTATION_DAYS)
    os.makedirs(JSON_ARCHIVE_DIR, exist_ok=True)
    try:
        for folder_name in os.listdir(JSON_BACKUP_DIR):
            try:
                folder_date = datetime.strptime(folder_name, '%Y-%m-%d')
                if folder_date < cutoff_date:
                    source_path = os.path.join(JSON_BACKUP_DIR, folder_name)
                    destination_path = os.path.join(JSON_ARCHIVE_DIR, folder_name)
                    if os.path.exists(destination_path): shutil.rmtree(destination_path)
                    shutil.move(source_path, destination_path)
                    print(f"ç§»å‹•ã—ã¾ã—ãŸ: {source_path}")
            except (ValueError, FileNotFoundError): continue
        print("âœ… å¤ã„ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–å®Œäº†ã€‚")
    except Exception as e:
        print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")

def main():
    print("ğŸš€ ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’é–‹å§‹ã—ã¾ã™ã€‚")
    
    # --- 1. EPGåŸºæœ¬æƒ…å ±ã®å–å¾— ---
    epg_data_to_upsert = []
    processed_event_ids = set()
    target_dates = [(datetime.now() + timedelta(days=i)) for i in range(-1, 8)]

    print("\n--- EPGåŸºæœ¬æƒ…å ±ã®å–å¾—é–‹å§‹ ---")
    for ch_type in ["td", "bs"]:
        for target_date in target_dates:
            date_str_url = target_date.strftime("%Y%m%d")
            date_str_db = target_date.strftime("%Y-%m-%d")
            url = f"https://bangumi.org/epg/{ch_type}?broad_cast_date={date_str_url}"
            if ch_type == "td": url += "&ggm_group_id=42"
            
            print(f"ã‚¢ã‚¯ã‚»ã‚¹ä¸­: {url}")
            try:
                res = requests.get(url, timeout=20)
                res.raise_for_status()
                soup = BeautifulSoup(res.text, 'html.parser')
                channel_tags = soup.find_all("li", class_="js_channel topmost")
                channel_names = [tag.text.strip() for tag in channel_tags]
                
                program_lines = soup.find_all("ul", id=lambda x: x and x.startswith("program_line_"))
                for i, line in enumerate(program_lines):
                    programs = line.find_all("li")
                    for program_tag in programs:
                        a_tag = program_tag.find("a", class_="title_link")
                        if not a_tag: continue
                        
                        href = a_tag.get("href", "")
                        event_id = href.split("/")[-1].split("?")[0]
                        if not event_id: continue

                        if event_id not in processed_event_ids:
                            channel_name = channel_names[i] if i < len(channel_names) else "ä¸æ˜"
                            channel_code = next((code for code, name_part in CHANNEL_MAPPING.items() if name_part in channel_name), None)
                            
                            epg_data_to_upsert.append({
                                "event_id": event_id, "broadcast_date": date_str_db,
                                "channel": channel_name, "start_time": str(program_tag.get("s", "")),
                                "end_time": str(program_tag.get("e", "")), "program_title": a_tag.find("p", class_="program_title").text.strip(),
                                "program_detail": a_tag.find("p", class_="program_detail").text.strip(),
                                "link": "https://bangumi.org" + href, "region": "æ±äº¬",
                                "channel_code": channel_code
                            })
                            processed_event_ids.add(event_id)
            except Exception as e:
                print(f"  -> EPGãƒšãƒ¼ã‚¸å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
                continue
    
    if not epg_data_to_upsert:
        raise Exception("EPGæƒ…å ±ãŒä¸€ä»¶ã‚‚å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚å‡¦ç†ã‚’ä¸­æ–­ã—ã¾ã™ã€‚")
        
    print(f"\nâœ… {len(epg_data_to_upsert)}ä»¶ã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªEPGæƒ…å ±ã‚’å–å¾—ã€‚DBã«ç™»éŒ²ã—ã¾ã™...")
    supabase.table('programs_epg').upsert(epg_data_to_upsert, on_conflict='event_id').execute()

    # --- 2. ç•ªçµ„è©³ç´°æƒ…å ±ã®å–å¾— ---
    print("\n--- ç•ªçµ„è©³ç´°æƒ…å ±ã®å–å¾—é–‹å§‹ ---")
    program_details_to_upsert = []
    
    for program in epg_data_to_upsert:
        if not program.get('link') or not program.get('channel_code') or program['channel_code'] not in TARGET_CHANNELS:
            continue
            
        print(f"è©³ç´°å–å¾—ä¸­: {program['program_title']}")
        try:
            res_detail = requests.get(program['link'], timeout=20)
            res_detail.raise_for_status()
            soup_detail = BeautifulSoup(res_detail.text, 'html.parser')
            
            title = program['program_title']
            meta_desc = soup_detail.find("meta", {"name": "description"})
            description = meta_desc["content"].strip() if meta_desc else ""
            letter_body = soup_detail.find("p", class_="letter_body")
            description_detail = letter_body.get_text(strip=True) if letter_body else ""
            genre_tag = soup_detail.find("p", class_="genre nomal")
            genre = genre_tag.get_text(strip=True).replace("\u3000", " ") if genre_tag else ""
            site_tag = soup_detail.select_one("ul.related_link a")
            official_website = site_tag.get("href") if site_tag else ""
            
            cast_text = ""
            for tag in soup_detail.find_all(["div", "p", "span"]):
                txt = tag.get_text(strip=True)
                if txt.startswith(("å‡ºæ¼”è€…", "ã€ã‚­ãƒ£ã‚¹ã‚¿ãƒ¼ã€‘")) or "èªã‚Š" in txt:
                    cast_text = txt
                    break
            
            cast_names = [c.strip() for c in cast_text.replace("ã€èªã‚Šã€‘", "").replace("ã€ã‚­ãƒ£ã‚¹ã‚¿ãƒ¼ã€‘", "").replace("ã€å‡ºæ¼”è€…ã€‘", "").replace("å‡ºæ¼”è€…", "").split("ï¼Œ") if c.strip()]
            
            performer_links = {}
            for a in soup_detail.select("a[href*='/talents/']"):
                name = a.get_text(strip=True)
                href = a.get("href", "")
                if name and href and not href.rstrip("/").endswith("talents"):
                    performer_links[name] = "https://bangumi.org" + href if href.startswith("/") else href
                                
            talents_to_upsert = []
            for name, link in performer_links.items():
                if not name or not link:
                    continue  # åå‰ã¾ãŸã¯ãƒªãƒ³ã‚¯ãŒç©ºãªã‚‰ã‚¹ã‚­ãƒƒãƒ—
            
                try:
                    # URLã®æœ€å¾Œã®éƒ¨åˆ†ã‹ã‚‰ talent_id ã‚’æŠ½å‡ºï¼ˆä¾‹: "/talents/172499" â†’ "172499"ï¼‰
                    talent_id = link.rstrip("/").split("/")[-1].split("?")[0]
                    
                    if talent_id.isdigit():
                        talents_to_upsert.append({
                            'talent_id': talent_id,
                            'name': name,
                            'link': link
                        })
                except Exception as e:
                    print(f"âš ï¸ ã‚¿ãƒ¬ãƒ³ãƒˆæƒ…å ±ã®è§£æã«å¤±æ•—ã—ã¾ã—ãŸ: name={name}, link={link}, error={e}")
                    continue
         
            appearances_to_insert = [{'program_event_id': program['event_id'], 'talent_id': talent['talent_id']} for talent in talents_to_upsert]

            db_data = {
                "event_id": program['event_id'], "broadcast_date": program['broadcast_date'],
                "channel": program['channel'], "start_time": program['start_time'],
                "end_time": program['end_time'], "master_title": title.split("ã€€")[0] if "ã€€" in title else title,
                "program_title": title, "description": description, "description_detail": description_detail,
                "genre": genre, "official_website": official_website, "channel_code": program['channel_code']
            }

            # (db_dataã«ãƒ‡ãƒ¼ã‚¿ã‚’è©°ã‚ãŸå¾Œ...)
            program_details_to_upsert.append(db_data)

            # --- â–¼â–¼â–¼ ã“ã“ã‹ã‚‰ä¿®æ­£ â–¼â–¼â–¼ ---
            # ã‚¯ãƒ©ã‚¦ãƒ‰ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã«ä¿å­˜ã™ã‚‹ãƒ‘ã‚¹ã¨ãƒ•ã‚¡ã‚¤ãƒ«åã‚’å®šç¾©
            date_str = program['broadcast_date']
            start_time_str = program['start_time']
            start_hhmm = "0000"
            if len(start_time_str) >= 12:
                start_hhmm = start_time_str[8:12]

            safe_event_id = program['event_id'].split('?')[0]
            # ä¿å­˜ãƒ‘ã‚¹: (ãƒã‚±ãƒƒãƒˆå)/YYYY-MM-DD/CHANNEL_CODE/ãƒ•ã‚¡ã‚¤ãƒ«å.json
            storage_path = f"{date_str}/{program['channel_code']}/{date_str}-{start_hhmm}_{program['channel_code']}_{safe_event_id}.json"

            # JSONã«ä¿å­˜ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã«ã¯ã€é–¢é€£æƒ…å ±ã‚‚å¿µã®ãŸã‚å«ã‚ã‚‹
            json_save_data = {**db_data, "performers": talents_to_upsert}
            # æ–‡å­—åˆ—ã«å¤‰æ›
            json_string = json.dumps(json_save_data, ensure_ascii=False, indent=2)

            # Supabase Storageã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
            try:
                # ãƒã‚±ãƒƒãƒˆåã¯ã‚¹ãƒ†ãƒƒãƒ—1ã§ä½œæˆã—ãŸã‚‚ã®ã«åˆã‚ã›ã¦ãã ã•ã„
                supabase.storage.from_('json-backups').upload(
                    path=storage_path,
                    file=json_string.encode('utf-8'),
                    file_options={"content-type": "application/json;charset=utf-8", "upsert": "true"}
                )
                print(f"  -> ã‚¯ãƒ©ã‚¦ãƒ‰JSONä¿å­˜å®Œäº†: {storage_path}")
            except Exception as storage_e:
                print(f"  -> âš ï¸ ã‚¯ãƒ©ã‚¦ãƒ‰JSONä¿å­˜ã‚¨ãƒ©ãƒ¼: {storage_e}")

            time.sleep(random.uniform(1.5, 2.5))
        except Exception as e:
            print(f"  -> è©³ç´°å–å¾—ã‚¨ãƒ©ãƒ¼: {program['program_title']} - {e}")
            continue

    if program_details_to_upsert:
        print(f"\nâœ… {len(program_details_to_upsert)}ä»¶ã®è©³ç´°æƒ…å ±ã‚’å–å¾—ã€‚DBã«ç™»éŒ²ã—ã¾ã™...")
        supabase.table('programs').upsert(program_details_to_upsert, on_conflict='event_id').execute()
    
    print("\nğŸ‰ å…¨ã¦ã®å‡¦ç†ãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸã€‚")
    
    # â–¼â–¼â–¼ ã“ã®ä¸€è¡Œã‚’è¿½åŠ  â–¼â–¼â–¼
    return len(epg_data_to_upsert), len(program_details_to_upsert)
        
if __name__ == '__main__':
    # å‡¦ç†å¯¾è±¡ã®æ—¥ä»˜ç¯„å›²ã‚’å…ˆã«å®šç¾©
    start_date = (datetime.now() - timedelta(days=1)).strftime('%Y-%m-%d')
    end_date = (datetime.now() + timedelta(days=7)).strftime('%Y-%m-%d')
    
    try:
        # mainé–¢æ•°ã‹ã‚‰å‡¦ç†ä»¶æ•°ã‚’å—ã‘å–ã‚‹
        epg_count, detail_count = main()
        
        # ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–å‡¦ç†
        # archive_old_files()
        archive_old_db_records()
        
        # æˆåŠŸé€šçŸ¥ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ä½œæˆ
        success_message = (
            f"âœ… ç•ªçµ„è¡¨ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯æ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸã€‚\n\n"
            f"**å‡¦ç†æœŸé–“**: {start_date} ï½ {end_date}\n"
            f"**ç•ªçµ„æ¦‚è¦**: {epg_count}ä»¶ å–å¾—\n"
            f"**ç•ªçµ„è©³ç´°**: {detail_count}ä»¶ å–å¾—"
        )
        send_discord_notification(success_message)
        

    except Exception as e:
        error_message = f"ğŸš¨ ç•ªçµ„è¡¨ã‚¹ã‚¯ãƒªãƒ—ãƒˆã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚\n\n**ã‚¨ãƒ©ãƒ¼å†…å®¹**:\n```\n{e}\n```"
        print(error_message)
        send_discord_notification(error_message)
