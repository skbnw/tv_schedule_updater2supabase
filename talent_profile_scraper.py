# talent_profile_scraper.py
# ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒªãƒã‚¸ãƒˆãƒªã®ãƒ«ãƒ¼ãƒˆã«ä¿å­˜ã—ã¦ãã ã•ã„

import requests
import time
import re
import os
import json
from bs4 import BeautifulSoup
from supabase import create_client, Client
from datetime import datetime
from typing import Dict, List, Optional
import logging

# ç’°å¢ƒå¤‰æ•°ã‹ã‚‰è¨­å®šã‚’å–å¾—
SUPABASE_URL = os.environ.get("SUPABASE_URL")
SUPABASE_KEY = os.environ.get("SUPABASE_KEY") 
DISCORD_WEBHOOK_URL = os.environ.get("DISCORD_WEBHOOK_URL")

# Supabaseæ¥ç¶š
supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)

class TalentProfileScraper:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        
        self.errors = []
        self.stats = {
            'total': 0,
            'success': 0,
            'failed': 0,
            'skipped': 0
        }
        
        # ãƒ­ã‚°è¨­å®š
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
    
    def get_talents_to_process(self, offset: int = 0, limit: int = 50):
        """å‡¦ç†å¯¾è±¡ã®ã‚¿ãƒ¬ãƒ³ãƒˆã‚’å–å¾—"""
        
        try:
            # æ—¢å­˜ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ãŒãªã„ã‚¿ãƒ¬ãƒ³ãƒˆã®ã¿å–å¾—
            query = '''
            SELECT t.talent_id, t.name, t.link 
            FROM talents t 
            LEFT JOIN talent_profiles tp ON t.talent_id = tp.talent_id 
            WHERE tp.talent_id IS NULL 
            ORDER BY t.talent_id 
            LIMIT {} OFFSET {}
            '''.format(limit, offset)
            
            # SQLã‚¯ã‚¨ãƒªå®Ÿè¡Œï¼ˆå®Ÿéš›ã®Supabaseã§ã¯é©åˆ‡ãªã‚¯ã‚¨ãƒªãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨ï¼‰
            talents = supabase.table('talents').select('talent_id, name, link').limit(limit).offset(offset).execute()
            
            self.logger.info(f"å‡¦ç†å¯¾è±¡: {len(talents.data)}ä»¶ (ã‚ªãƒ•ã‚»ãƒƒãƒˆ: {offset})")
            return talents.data
            
        except Exception as e:
            self.logger.error(f"ã‚¿ãƒ¬ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return []
    
    def scrape_talent_profile(self, talent_id: str, talent_link: str, talent_name: str) -> Optional[Dict]:
        """å€‹åˆ¥ã‚¿ãƒ¬ãƒ³ãƒˆã®ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«å–å¾—"""
        
        try:
            self.logger.info(f"å–å¾—ä¸­: {talent_name} (ID: {talent_id})")
            
            response = self.session.get(talent_link, timeout=15)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # åŸºæœ¬ãƒ‡ãƒ¼ã‚¿æ§‹é€ 
            profile_data = {
                'talent_id': talent_id,
                'source_url': talent_link,
                'scraped_at': datetime.now().isoformat()
            }
            
            # åå‰ã¨èª­ã¿æ–¹ã‚’æŠ½å‡º
            self._extract_name_info(soup, profile_data)
            
            # åŸºæœ¬æƒ…å ±ã‚’æŠ½å‡º
            self._extract_basic_info(soup, profile_data)
            
            # ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ç”»åƒ
            img_element = soup.find('img', class_='talent_img')
            if img_element and img_element.get('src'):
                profile_data['profile_image_url'] = img_element['src']
            
            # ã‚¸ãƒ£ãƒ³ãƒ«ã€ç‰¹æŠ€ã€è¶£å‘³ã€èŠ¸æ­´ã‚’æŠ½å‡º
            self._extract_profile_details(soup, profile_data)
            
            # å®Œæˆåº¦ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
            profile_data['profile_completeness'] = self._calculate_completeness(profile_data)
            
            return profile_data
            
        except Exception as e:
            error_info = {
                'talent_id': str(talent_id),
                'talent_name': talent_name,
                'error': str(e),
                'timestamp': datetime.now().isoformat()
            }
            self.errors.append(error_info)
            self.logger.error(f"ã‚¨ãƒ©ãƒ¼ - {talent_name}: {str(e)}")
            return None
    
    def _extract_name_info(self, soup, profile_data):
        """åå‰æƒ…å ±ã‚’æŠ½å‡º"""
        name_element = soup.find('li', text=re.compile(r'åå‰ï¼š'))
        if name_element:
            name_text = name_element.get_text()
            name_match = re.search(r'åå‰ï¼š\s*(.+?)ï¼ˆ(.+?)ï¼‰', name_text)
            if name_match:
                profile_data['full_name'] = name_match.group(1).strip()
                profile_data['reading'] = name_match.group(2).strip()
    
    def _extract_basic_info(self, soup, profile_data):
        """åŸºæœ¬æƒ…å ±ã‚’æŠ½å‡º"""
        # æƒ…å ±è¦ç´ ã‚’æ¤œç´¢
        info_element = soup.find('li', string=re.compile(r'æƒ…å ±ï¼š'))
        if not info_element:
            info_spans = soup.find_all('span', string=re.compile(r'æƒ…å ±ï¼š'))
            if info_spans:
                info_element = info_spans[0].parent
        
        if info_element:
            info_text = info_element.get_text()
            
            # ç”Ÿå¹´æœˆæ—¥
            birth_match = re.search(r'(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥', info_text)
            if birth_match:
                year, month, day = birth_match.groups()
                profile_data['birth_date'] = f"{year}-{month.zfill(2)}-{day.zfill(2)}"
            
            # æ˜Ÿåº§
            zodiac_match = re.search(r'(ãŠã²ã¤ã˜åº§|ãŠã†ã—åº§|ãµãŸã”åº§|ã‹ã«åº§|ã—ã—åº§|ãŠã¨ã‚åº§|ã¦ã‚“ã³ã‚“åº§|ã•ãã‚Šåº§|ã„ã¦åº§|ã‚„ãåº§|ã¿ãšãŒã‚åº§|ã†ãŠåº§)', info_text)
            if zodiac_match:
                profile_data['zodiac_sign'] = zodiac_match.group(1)
            
            # è¡€æ¶²å‹
            blood_match = re.search(r'([ABO]B?)å‹', info_text)
            if blood_match:
                profile_data['blood_type'] = blood_match.group(1) + 'å‹'
            
            # èº«é•·
            height_match = re.search(r'(\d+)cm', info_text)
            if height_match:
                profile_data['height_cm'] = int(height_match.group(1))
            
            # å‡ºèº«åœ°
            birthplace_match = re.search(r'([^0-9]+?)å‡ºèº«', info_text)
            if birthplace_match:
                profile_data['birthplace'] = birthplace_match.group(1).strip()
    
    def _extract_profile_details(self, soup, profile_data):
        """ã‚¸ãƒ£ãƒ³ãƒ«ã€ç‰¹æŠ€ã€è¶£å‘³ã€èŠ¸æ­´ã‚’æŠ½å‡º"""
        
        # ã‚¸ãƒ£ãƒ³ãƒ«
        genre_element = soup.find('p', id='ã‚¸ãƒ£ãƒ³ãƒ«')
        if not genre_element:
            genre_spans = soup.find_all('span', string=re.compile(r'ã‚¸ãƒ£ãƒ³ãƒ«ï¼š'))
            if genre_spans:
                genre_element = genre_spans[0].find_next('p')
        
        if genre_element:
            genre_text = genre_element.get_text().strip()
            genres = [g.strip() for g in re.split(r'[\s\u3000\n]+', genre_text) if g.strip()]
            profile_data['genres'] = genres
        
        # ç‰¹æŠ€
        skill_element = soup.find('p', id='ç‰¹æŠ€')
        if not skill_element:
            skill_spans = soup.find_all('span', string=re.compile(r'ç‰¹æŠ€ï¼š'))
            if skill_spans:
                skill_element = skill_spans[0].find_next('p')
        
        if skill_element:
            skill_text = skill_element.get_text().strip()
            skills = [s.strip() for s in re.split(r'[\s\u3000\n]+', skill_text.replace('ã€€', ' ')) if s.strip()]
            profile_data['skills'] = skills
        
        # è¶£å‘³
        hobby_element = soup.find('p', id='è¶£å‘³')
        if not hobby_element:
            hobby_spans = soup.find_all('span', string=re.compile(r'è¶£å‘³ï¼š'))
            if hobby_spans:
                hobby_element = hobby_spans[0].find_next('p')
        
        if hobby_element:
            hobby_text = hobby_element.get_text().strip()
            hobbies = [h.strip() for h in re.split(r'[\s\u3000\n]+', hobby_text.replace('ã€€', ' ')) if h.strip()]
            profile_data['hobbies'] = hobbies
        
        # èŠ¸æ­´
        career_element = soup.find('p', id='èŠ¸æ­´')
        if not career_element:
            career_spans = soup.find_all('span', string=re.compile(r'èŠ¸æ­´ï¼š'))
            if career_spans:
                career_element = career_spans[0].find_next('p')
        
        if career_element:
            profile_data['career_history'] = career_element.get_text().strip()
    
    def _calculate_completeness(self, profile_data: Dict) -> float:
        """ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«å®Œæˆåº¦ã‚’è¨ˆç®—"""
        score = 0.0
        max_score = 10.0
        
        if profile_data.get('full_name'): score += 1.0
        if profile_data.get('reading'): score += 0.5
        if profile_data.get('birth_date'): score += 1.5
        if profile_data.get('birthplace'): score += 1.0
        if profile_data.get('profile_image_url'): score += 0.5
        if profile_data.get('genres'): score += 2.0
        if profile_data.get('skills'): score += 1.0
        if profile_data.get('hobbies'): score += 1.0
        if profile_data.get('career_history'): score += 1.5
        
        return min(1.0, score / max_score)
    
    def save_profile(self, profile_data: Dict) -> bool:
        """ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜"""
        
        try:
            result = supabase.table('talent_profiles').upsert(profile_data).execute()
            
            # ã‚¿ã‚°ç”Ÿæˆã¨ä¿å­˜
            tags = self._generate_tags(profile_data)
            if tags:
                self._save_tags(tags)
            
            self.logger.info(f"ä¿å­˜æˆåŠŸ: ID {profile_data['talent_id']} (å®Œæˆåº¦: {profile_data.get('profile_completeness', 0):.2f})")
            return True
            
        except Exception as e:
            self.logger.error(f"ä¿å­˜ã‚¨ãƒ©ãƒ¼: ID {profile_data['talent_id']} - {str(e)}")
            return False
    
    def _generate_tags(self, profile_data: Dict) -> List[Dict]:
        """ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ã‹ã‚‰ã‚¿ã‚°ã‚’ç”Ÿæˆ"""
        tags = []
        talent_id = str(profile_data['talent_id'])  # æ–‡å­—åˆ—ã¨ã—ã¦æ‰±ã†
        
        # ã‚¸ãƒ£ãƒ³ãƒ«ã‹ã‚‰ã‚¿ã‚°ç”Ÿæˆ
        if 'genres' in profile_data:
            for genre in profile_data['genres']:
                tag_name = self._normalize_genre_tag(genre)
                if tag_name:
                    tags.append({
                        'talent_id': talent_id,
                        'tag_name': tag_name,
                        'tag_category': 'profession',
                        'confidence_score': 1.0
                    })
        
        return tags
    
    def _normalize_genre_tag(self, genre: str) -> Optional[str]:
        """ã‚¸ãƒ£ãƒ³ãƒ«ã‚’ã‚¿ã‚°ã«æ­£è¦åŒ–"""
        
        mapping = {
            'å¥³å„ª': 'actress',
            'ä¿³å„ª': 'actor',
            'ã‚¿ãƒ¬ãƒ³ãƒˆ': 'talent',
            'æ­Œæ‰‹': 'singer',
            'å£°å„ª': 'voice_actor',
            'ã‚¢ãƒŠã‚¦ãƒ³ã‚µãƒ¼': 'announcer',
            'NHKã‚¢ãƒŠã‚¦ãƒ³ã‚µãƒ¼': 'nhk_announcer',
            'æ”¿æ²»å®¶': 'politician',
            'ãŠç¬‘ã„èŠ¸äºº': 'comedian',
            'ãƒ¢ãƒ‡ãƒ«': 'model'
        }
        
        return mapping.get(genre)
    
    def _save_tags(self, tags: List[Dict]):
        """ã‚¿ã‚°ã‚’ä¿å­˜"""
        # ã‚¿ã‚°ä¿å­˜ã®å®Ÿè£…
        # å®Ÿéš›ã®å®Ÿè£…ã§ã¯talent_tagsãƒ†ãƒ¼ãƒ–ãƒ«ã‚‚å¿…è¦
        pass
    
    def send_discord_notification(self):
        """Discordé€šçŸ¥ã‚’é€ä¿¡"""
        
        if not DISCORD_WEBHOOK_URL:
            return
        
        try:
            embed = {
                "title": "ğŸ­ ã‚¿ãƒ¬ãƒ³ãƒˆãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«å–å¾—å®Œäº†",
                "color": 0x00ff00 if self.stats['failed'] == 0 else 0xff9900,
                "fields": [
                    {"name": "ğŸ“Š å‡¦ç†çµæœ", "value": f"```æˆåŠŸ: {self.stats['success']}ä»¶\nå¤±æ•—: {self.stats['failed']}ä»¶\nåˆè¨ˆ: {self.stats['total']}ä»¶```"},
                    {"name": "ğŸ“ˆ æˆåŠŸç‡", "value": f"{(self.stats['success'] / max(1, self.stats['total'])) * 100:.1f}%"}
                ],
                "timestamp": datetime.now().isoformat()
            }
            
            payload = {"embeds": [embed]}
            requests.post(DISCORD_WEBHOOK_URL, json=payload)
            
        except Exception as e:
            self.logger.error(f"Discordé€šçŸ¥ã‚¨ãƒ©ãƒ¼: {str(e)}")
    
    def save_error_log(self):
        """ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ã‚’ä¿å­˜"""
        
        if not self.errors:
            return
        
        filename = f"talent_scraping_errors_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        error_data = {
            'timestamp': datetime.now().isoformat(),
            'total_errors': len(self.errors),
            'stats': self.stats,
            'errors': self.errors
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(error_data, f, ensure_ascii=False, indent=2)
        
        self.logger.info(f"ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ä¿å­˜: {filename}")
    
    def process_talents(self, offset: int = 0, limit: int = 50):
        """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
        
        start_time = datetime.now()
        self.logger.info(f"ã‚¿ãƒ¬ãƒ³ãƒˆãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«å–å¾—é–‹å§‹ (ã‚ªãƒ•ã‚»ãƒƒãƒˆ: {offset}, ä»¶æ•°: {limit})")
        
        # å‡¦ç†å¯¾è±¡ã‚’å–å¾—
        talents = self.get_talents_to_process(offset, limit)
        self.stats['total'] = len(talents)
        
        # å„ã‚¿ãƒ¬ãƒ³ãƒˆã‚’å‡¦ç†
        for i, talent in enumerate(talents):
            talent_id = talent['talent_id']
            talent_name = talent['name']
            talent_link = talent['link']
            
            self.logger.info(f"[{i + 1}/{len(talents)}] å‡¦ç†ä¸­: {talent_name}")
            
            # ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«å–å¾—
            profile_data = self.scrape_talent_profile(talent_id, talent_link, talent_name)
            
            if profile_data:
                # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¿å­˜
                if self.save_profile(profile_data):
                    self.stats['success'] += 1
                else:
                    self.stats['failed'] += 1
            else:
                self.stats['failed'] += 1
            
            # ãƒ¬ãƒ¼ãƒˆåˆ¶é™
            time.sleep(1.5)
        
        # å®Œäº†å‡¦ç†
        end_time = datetime.now()
        execution_time = (end_time - start_time).total_seconds() / 60
        
        self.logger.info(f"å‡¦ç†å®Œäº†: {execution_time:.1f}åˆ†")
        self.logger.info(f"çµæœ: æˆåŠŸ {self.stats['success']}ä»¶, å¤±æ•— {self.stats['failed']}ä»¶")
        
        # ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ä¿å­˜
        self.save_error_log()
        
        # Discordé€šçŸ¥
        self.send_discord_notification()

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    
    import argparse
    
    parser = argparse.ArgumentParser(description='ã‚¿ãƒ¬ãƒ³ãƒˆãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«å–å¾—')
    parser.add_argument('--mode', choices=['test', 'batch', 'full'], default='test',
                       help='å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰')
    parser.add_argument('--offset', type=int, default=0,
                       help='å‡¦ç†é–‹å§‹ã‚ªãƒ•ã‚»ãƒƒãƒˆ')
    
    args = parser.parse_args()
    
    # å‡¦ç†ä»¶æ•°ã‚’æ±ºå®š
    if args.mode == 'test':
        limit = 10
    elif args.mode == 'batch':
        limit = 50
    else:  # full
        limit = 1000  # å¤§é‡å‡¦ç†æ™‚ã®ä¸Šé™
    
    # å‡¦ç†å®Ÿè¡Œ
    scraper = TalentProfileScraper()
    scraper.process_talents(offset=args.offset, limit=limit)

if __name__ == "__main__":
    main()
