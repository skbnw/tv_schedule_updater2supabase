# å„ªå…ˆåº¦é«˜ã®ä¿®æ­£ã‚’é©ç”¨ã—ãŸãƒãƒ¼ã‚¸ãƒ§ãƒ³

import requests
import time
import re
import os
import json
from bs4 import BeautifulSoup
from supabase import create_client, Client
from datetime import datetime
from typing import Dict, List, Optional
import logging

# ç’°å¢ƒå¤‰æ•°ã‹ã‚‰è¨­å®šã‚’å–å¾—
SUPABASE_URL = os.environ.get("SUPABASE_URL")
SUPABASE_KEY = os.environ.get("SUPABASE_KEY") 
DISCORD_WEBHOOK_URL = os.environ.get("DISCORD_WEBHOOK_URL")

# Supabaseæ¥ç¶š
supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)

class TalentProfileScraperFixed:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        
        self.errors = []
        self.stats = {
            'total': 0,
            'success': 0,
            'failed': 0,
            'skipped': 0
        }
        
        # ãƒ­ã‚°è¨­å®š
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
    
    def get_talents_to_process(self, offset: int = 0, limit: int = 50):
        """å‡¦ç†å¯¾è±¡ã®ã‚¿ãƒ¬ãƒ³ãƒˆã‚’å–å¾—ï¼ˆæ—¢å­˜ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ã‚’é™¤å¤–ï¼‰"""
        
        try:
            # æ—¢å­˜ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ãŒãªã„ã‚¿ãƒ¬ãƒ³ãƒˆã®ã¿å–å¾—
            existing_profiles = supabase.table('talent_profiles').select('talent_id').execute()
            existing_ids = {row['talent_id'] for row in existing_profiles.data}
            
            # å…¨ã‚¿ãƒ¬ãƒ³ãƒˆã‚’å–å¾—
            all_talents = supabase.table('talents').select('talent_id, name, link').limit(limit + len(existing_ids)).offset(offset).execute()
            
            # æ—¢å­˜ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ãŒãªã„ã‚¿ãƒ¬ãƒ³ãƒˆã®ã¿ãƒ•ã‚£ãƒ«ã‚¿
            talents_to_process = [t for t in all_talents.data if t['talent_id'] not in existing_ids][:limit]
            
            self.logger.info(f"å‡¦ç†å¯¾è±¡: {len(talents_to_process)}ä»¶ (ã‚ªãƒ•ã‚»ãƒƒãƒˆ: {offset}, æ—¢å­˜é™¤å¤–: {len(existing_ids)}ä»¶)")
            return talents_to_process
            
        except Exception as e:
            self.logger.error(f"ã‚¿ãƒ¬ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return []
    
    def scrape_talent_profile(self, talent_id: str, talent_link: str, talent_name: str) -> Optional[Dict]:
        """å€‹åˆ¥ã‚¿ãƒ¬ãƒ³ãƒˆã®ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«å–å¾—ï¼ˆä¿®æ­£ç‰ˆï¼‰"""
        
        try:
            self.logger.info(f"å–å¾—ä¸­: {talent_name} (ID: {talent_id})")
            
            response = self.session.get(talent_link, timeout=15)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # åŸºæœ¬ãƒ‡ãƒ¼ã‚¿æ§‹é€ 
            profile_data = {
                'talent_id': talent_id,
                'source_url': talent_link,
                'scraped_at': datetime.now().isoformat()
            }
            
            # åå‰ã¨èª­ã¿æ–¹ã‚’æŠ½å‡ºï¼ˆä¿®æ­£ç‰ˆï¼‰
            self._extract_name_info_fixed(soup, profile_data)
            
            # åŸºæœ¬æƒ…å ±ã‚’æŠ½å‡ºï¼ˆä¿®æ­£ç‰ˆï¼‰
            self._extract_basic_info_fixed(soup, profile_data)
            
            # ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ç”»åƒ
            img_element = soup.find('img', class_='talent_img')
            if img_element and img_element.get('src'):
                profile_data['profile_image_url'] = img_element['src']
            
            # ã‚¸ãƒ£ãƒ³ãƒ«ã€ç‰¹æŠ€ã€è¶£å‘³ã€èŠ¸æ­´ã‚’æŠ½å‡ºï¼ˆä¿®æ­£ç‰ˆï¼‰
            self._extract_profile_details_fixed(soup, profile_data)
            
            # å®Œæˆåº¦ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
            profile_data['profile_completeness'] = self._calculate_completeness(profile_data)
            
            return profile_data
            
        except Exception as e:
            error_info = {
                'talent_id': str(talent_id),
                'talent_name': talent_name,
                'error': str(e),
                'timestamp': datetime.now().isoformat()
            }
            self.errors.append(error_info)
            self.logger.error(f"ã‚¨ãƒ©ãƒ¼ - {talent_name}: {str(e)}")
            return None
    
    def _extract_name_info_fixed(self, soup, profile_data):
        """åå‰æƒ…å ±ã‚’æŠ½å‡ºï¼ˆä¿®æ­£ç‰ˆï¼‰"""
        
        # ä¿®æ­£1: stringå¼•æ•°ã‚’ä½¿ç”¨ï¼ˆtextå¼•æ•°ã¯éæ¨å¥¨ï¼‰
        name_element = soup.find('li', string=re.compile(r'åå‰ï¼š'))
        
        # ä¿®æ­£2: è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯spanæ¤œç´¢
        if not name_element:
            name_spans = soup.find_all('span', string=re.compile(r'åå‰ï¼š'))
            if name_spans:
                name_element = name_spans[0].parent
        
        # ä¿®æ­£3: ã‚ˆã‚ŠæŸ”è»Ÿãªåå‰ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°
        if name_element:
            name_text = name_element.get_text()
            
            # ãƒ‘ã‚¿ãƒ¼ãƒ³1: åå‰ï¼šå±±ä¹‹å†… ã™ãšï¼ˆãƒ¤ãƒãƒã‚¦ãƒ ã‚¹ã‚ºï¼‰
            name_match = re.search(r'åå‰[ï¼š:]\s*(.+?)ï¼ˆ(.+?)ï¼‰', name_text)
            if name_match:
                profile_data['full_name'] = name_match.group(1).strip()
                profile_data['reading'] = name_match.group(2).strip()
            else:
                # ãƒ‘ã‚¿ãƒ¼ãƒ³2: åå‰ã®ã¿ã®å ´åˆ
                simple_match = re.search(r'åå‰[ï¼š:]\s*(.+)', name_text)
                if simple_match:
                    name_only = simple_match.group(1).strip()
                    # èª­ã¿æ–¹ãŒå«ã¾ã‚Œã¦ã„ãªã„å ´åˆ
                    if not 'ï¼ˆ' in name_only:
                        profile_data['full_name'] = name_only
    
    def _extract_basic_info_fixed(self, soup, profile_data):
        """åŸºæœ¬æƒ…å ±ã‚’æŠ½å‡ºï¼ˆä¿®æ­£ç‰ˆï¼‰"""
        
        # ä¿®æ­£: stringå¼•æ•°ã‚’ä½¿ç”¨
        info_element = soup.find('li', string=re.compile(r'æƒ…å ±ï¼š'))
        if not info_element:
            info_spans = soup.find_all('span', string=re.compile(r'æƒ…å ±ï¼š'))
            if info_spans:
                info_element = info_spans[0].parent
        
        if info_element:
            info_text = info_element.get_text()
            
            # ç”Ÿå¹´æœˆæ—¥
            birth_match = re.search(r'(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥', info_text)
            if birth_match:
                year, month, day = birth_match.groups()
                profile_data['birth_date'] = f"{year}-{month.zfill(2)}-{day.zfill(2)}"
            
            # æ˜Ÿåº§
            zodiac_match = re.search(r'(ãŠã²ã¤ã˜åº§|ãŠã†ã—åº§|ãµãŸã”åº§|ã‹ã«åº§|ã—ã—åº§|ãŠã¨ã‚åº§|ã¦ã‚“ã³ã‚“åº§|ã•ãã‚Šåº§|ã„ã¦åº§|ã‚„ãåº§|ã¿ãšãŒã‚åº§|ã†ãŠåº§)', info_text)
            if zodiac_match:
                profile_data['zodiac_sign'] = zodiac_match.group(1)
            
            # è¡€æ¶²å‹
            blood_match = re.search(r'([ABO]B?)å‹', info_text)
            if blood_match:
                profile_data['blood_type'] = blood_match.group(1) + 'å‹'
            
            # èº«é•·
            height_match = re.search(r'(\d+)cm', info_text)
            if height_match:
                profile_data['height_cm'] = int(height_match.group(1))
            
            # å‡ºèº«åœ°ï¼ˆä¿®æ­£ç‰ˆï¼šãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ï¼‰
            birthplace_match = re.search(r'([^0-9]+?)å‡ºèº«', info_text)
            if birthplace_match:
                raw_birthplace = birthplace_match.group(1).strip()
                # ä¿®æ­£: ä¸è¦ãªæ–‡å­—åˆ—ã‚’é™¤å»
                cleaned_birthplace = self._clean_birthplace(raw_birthplace)
                if cleaned_birthplace:
                    profile_data['birthplace'] = cleaned_birthplace
    
    def _clean_birthplace(self, raw_text: str) -> str:
        """å‡ºèº«åœ°ãƒ‡ãƒ¼ã‚¿ã®ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°"""
        
        # ä¸è¦ãªæ–‡å­—åˆ—ã‚’é™¤å»
        cleaned = re.sub(r'(cm|kg)\s*\n\s*', '', raw_text)
        cleaned = re.sub(r'\s+', '', cleaned)  # ä½™åˆ†ãªç©ºç™½é™¤å»
        cleaned = re.sub(r'^[æ—¥æœˆç«æ°´æœ¨é‡‘åœŸ]+\s*', '', cleaned)  # æ›œæ—¥æ–‡å­—é™¤å»
        
        # ç©ºæ–‡å­—ã‚„æ„å‘³ã®ãªã„æ–‡å­—åˆ—ã‚’ãƒ•ã‚£ãƒ«ã‚¿
        if len(cleaned) < 2 or cleaned in ['æ—¥', 'æœˆ', 'ç«', 'æ°´', 'æœ¨', 'é‡‘', 'åœŸ']:
            return ''
        
        return cleaned.strip()
    
    def _extract_profile_details_fixed(self, soup, profile_data):
        """ã‚¸ãƒ£ãƒ³ãƒ«ã€ç‰¹æŠ€ã€è¶£å‘³ã€èŠ¸æ­´ã‚’æŠ½å‡ºï¼ˆä¿®æ­£ç‰ˆï¼‰"""
        
        # ä¿®æ­£: stringå¼•æ•°ã‚’ä½¿ç”¨
        detail_fields = {
            'ã‚¸ãƒ£ãƒ³ãƒ«': 'genres',
            'ç‰¹æŠ€': 'skills', 
            'è¶£å‘³': 'hobbies',
            'èŠ¸æ­´': 'career_history'
        }
        
        for japanese_key, english_key in detail_fields.items():
            # idå±æ€§ã§æ¤œç´¢
            element = soup.find('p', id=japanese_key)
            
            # idå±æ€§ãŒãªã„å ´åˆã¯spanæ¤œç´¢ï¼ˆä¿®æ­£ç‰ˆï¼‰
            if not element:
                spans = soup.find_all('span', string=re.compile(f'{japanese_key}[ï¼š:]'))
                if spans:
                    element = spans[0].find_next('p')
            
            if element:
                text = element.get_text().strip()
                
                if english_key == 'career_history':
                    profile_data[english_key] = text
                else:
                    # ãƒªã‚¹ãƒˆå½¢å¼ã§ä¿å­˜ï¼ˆä¿®æ­£ç‰ˆï¼šã‚ˆã‚Šæ­£ç¢ºãªåˆ†å‰²ï¼‰
                    items = self._split_detail_items(text)
                    if items:
                        profile_data[english_key] = items
    
    def _split_detail_items(self, text: str) -> List[str]:
        """è©³ç´°é …ç›®ã®åˆ†å‰²å‡¦ç†"""
        
        # è¤‡æ•°ã®åŒºåˆ‡ã‚Šæ–‡å­—ã«å¯¾å¿œ
        items = re.split(r'[\s\u3000\n]+', text.replace('ã€€', ' '))
        
        # ç©ºæ–‡å­—ã‚„çŸ­ã™ãã‚‹é …ç›®ã‚’é™¤å»
        filtered_items = []
        for item in items:
            item = item.strip()
            if len(item) > 1 and item not in ['ã€', 'ï¼Œ', 'ãƒ»']:
                filtered_items.append(item)
        
        return filtered_items
    
    def _calculate_completeness(self, profile_data: Dict) -> float:
        """ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«å®Œæˆåº¦ã‚’è¨ˆç®—"""
        score = 0.0
        max_score = 10.0
        
        if profile_data.get('full_name'): score += 1.0
        if profile_data.get('reading'): score += 0.5
        if profile_data.get('birth_date'): score += 1.5
        if profile_data.get('birthplace'): score += 1.0
        if profile_data.get('profile_image_url'): score += 0.5
        if profile_data.get('genres'): score += 2.0
        if profile_data.get('skills'): score += 1.0
        if profile_data.get('hobbies'): score += 1.0
        if profile_data.get('career_history'): score += 1.5
        
        return min(1.0, score / max_score)
    
    def save_profile(self, profile_data: Dict) -> bool:
        """ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜"""
        
        try:
            result = supabase.table('talent_profiles').upsert(profile_data).execute()
            
            # ä¿®æ­£: ã‚¿ã‚°ç”Ÿæˆã¨ä¿å­˜ã‚’è¿½åŠ 
            tags = self._generate_tags(profile_data)
            if tags:
                self._save_tags_to_db(tags)
            
            self.logger.info(f"ä¿å­˜æˆåŠŸ: ID {profile_data['talent_id']} (å®Œæˆåº¦: {profile_data.get('profile_completeness', 0):.2f})")
            return True
            
        except Exception as e:
            self.logger.error(f"ä¿å­˜ã‚¨ãƒ©ãƒ¼: ID {profile_data['talent_id']} - {str(e)}")
            return False
    
    def _generate_tags(self, profile_data: Dict) -> List[Dict]:
        """ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ã‹ã‚‰ã‚¿ã‚°ã‚’ç”Ÿæˆï¼ˆå¼·åŒ–ç‰ˆï¼‰"""
        tags = []
        talent_id = str(profile_data['talent_id'])
        
        # ã‚¸ãƒ£ãƒ³ãƒ«ã‹ã‚‰ã‚¿ã‚°ç”Ÿæˆ
        if 'genres' in profile_data:
            for genre in profile_data['genres']:
                tag_name = self._normalize_genre_tag(genre)
                if tag_name:
                    tags.append({
                        'talent_id': talent_id,
                        'tag_name': tag_name,
                        'tag_category': 'profession',
                        'confidence_score': 1.0,
                        'extraction_method': 'genre'
                    })
        
        return tags
    
    def _normalize_genre_tag(self, genre: str) -> Optional[str]:
        """ã‚¸ãƒ£ãƒ³ãƒ«ã‚’ã‚¿ã‚°ã«æ­£è¦åŒ–ï¼ˆæ‹¡å¼µç‰ˆï¼‰"""
        
        mapping = {
            'å¥³å„ª': 'actress',
            'ä¿³å„ª': 'actor',
            'ã‚¿ãƒ¬ãƒ³ãƒˆ': 'talent',
            'æ­Œæ‰‹': 'singer',
            'å£°å„ªãƒ»ãƒŠãƒ¬ãƒ¼ã‚¿ãƒ¼': 'voice_actor',
            'å£°å„ª': 'voice_actor',
            'ã‚¢ãƒŠã‚¦ãƒ³ã‚µãƒ¼': 'announcer',
            'NHKã‚¢ãƒŠã‚¦ãƒ³ã‚µãƒ¼': 'nhk_announcer',
            'æ”¿æ²»å®¶': 'politician',
            'ãŠç¬‘ã„èŠ¸äºº': 'comedian',
            'ãŠç¬‘ã„ã‚¿ãƒ¬ãƒ³ãƒˆ': 'comedian',
            'ãƒ¢ãƒ‡ãƒ«': 'model',
            'å­å½¹': 'child_actor',
            'æ­Œæ‰‹ãƒ»ã‚¢ãƒ¼ãƒ†ã‚£ã‚¹ãƒˆ': 'singer',
            'æ°—è±¡äºˆå ±å£«': 'weather_forecaster',
            'é˜²ç½å£«': 'disaster_prevention_specialist',
            'ãƒ•ã‚¡ã‚¤ãƒŠãƒ³ã‚·ãƒ£ãƒ«ãƒ—ãƒ©ãƒ³ãƒŠãƒ¼': 'financial_planner',
            'ãƒ—ãƒ­ãƒ•ã‚£ã‚®ãƒ¥ã‚¢ã‚¹ã‚±ãƒ¼ã‚¿ãƒ¼': 'figure_skater'
        }
        
        return mapping.get(genre)
    
    def _save_tags_to_db(self, tags: List[Dict]):
        """ã‚¿ã‚°ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜ï¼ˆUPSERTã‚’æ­£ã—ãä½¿ç”¨ã™ã‚‹ä¿®æ­£ç‰ˆï¼‰"""
        
        for tag_data in tags:
            try:
                tag_name = tag_data['tag_name']
                tag_category = tag_data['tag_category']
                
                # 1. ã‚¿ã‚°ãƒã‚¹ã‚¿ãƒ¼ã«ç™»éŒ²ï¼ˆå­˜åœ¨ã—ãªã„å ´åˆã¯æŒ¿å…¥ï¼‰
                # on_conflictã§é‡è¤‡ã‚’ãƒã‚§ãƒƒã‚¯ã—ã€å­˜åœ¨ã™ã‚Œã°ä½•ã‚‚ã—ãªã„
                supabase.table('talent_tags').upsert(
                    {'tag_name': tag_name, 'tag_category': tag_category},
                    on_conflict='tag_name'  # 'tag_name'åˆ—ã§é‡è¤‡ã‚’åˆ¤æ–­
                ).execute()

                # 2. upsertå¾Œã«tag_idã‚’æ”¹ã‚ã¦å–å¾—
                tag_response = supabase.table('talent_tags').select('tag_id').eq('tag_name', tag_name).execute()
                
                # ã‚¿ã‚°ãŒå­˜åœ¨ã—ãªã„ã¨ã„ã†ç¨€ãªã‚±ãƒ¼ã‚¹ã‚’è€ƒæ…®
                if not tag_response.data:
                    self.logger.warning(f"ã‚¿ã‚°IDã®å–å¾—ã«å¤±æ•—: {tag_name}")
                    continue
                tag_id = tag_response.data[0]['tag_id']
                
                # 3. ã‚¿ãƒ¬ãƒ³ãƒˆã¨ã‚¿ã‚°ã®é–¢é€£ã‚’ä¿å­˜
                relation_data = {
                    'talent_id': tag_data['talent_id'],
                    'tag_id': tag_id,
                    'confidence_score': tag_data['confidence_score'],
                    'extraction_method': tag_data['extraction_method']
                }
                
                # ã“ã“ãŒæœ€ã‚‚é‡è¦ï¼è¤‡åˆã‚­ãƒ¼(talent_id, tag_id)ã§é‡è¤‡ã‚’åˆ¤æ–­ã™ã‚‹
                supabase.table('talent_tag_relations').upsert(
                    relation_data,
                    on_conflict='talent_id,tag_id' 
                ).execute()
                
            except Exception as e:
                # ã“ã®ä¾‹å¤–å‡¦ç†ã¯ã€ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¨ãƒ©ãƒ¼ãªã©äºˆæœŸã›ã¬å•é¡Œã®ãŸã‚ã«æ®‹ã—ã¦ãŠã
                self.logger.error(f"ã‚¿ã‚°ä¿å­˜ä¸­ã®äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼: {tag_data} - {str(e)}")
                
    def send_discord_notification(self):
        """Discordé€šçŸ¥ã‚’é€ä¿¡"""
        
        if not DISCORD_WEBHOOK_URL:
            return
        
        try:
            embed = {
                "title": "ğŸ­ Talent Profile Scraper Completed",
                "color": 0x00ff00 if self.stats['failed'] == 0 else 0xff9900,
                "fields": [
                    {"name": "ğŸ“Š Results", "value": f"```Success: {self.stats['success']}\nFailed: {self.stats['failed']}\nTotal: {self.stats['total']}```"},
                    {"name": "ğŸ“ˆ Success Rate", "value": f"{(self.stats['success'] / max(1, self.stats['total'])) * 100:.1f}%"}
                ],
                "timestamp": datetime.now().isoformat()
            }
            
            payload = {"embeds": [embed]}
            requests.post(DISCORD_WEBHOOK_URL, json=payload)
            
        except Exception as e:
            self.logger.error(f"Discordé€šçŸ¥ã‚¨ãƒ©ãƒ¼: {str(e)}")
    
    def save_error_log(self):
        """ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ã‚’ä¿å­˜"""
        
        if not self.errors:
            return
        
        filename = f"talent_scraping_errors_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        error_data = {
            'timestamp': datetime.now().isoformat(),
            'total_errors': len(self.errors),
            'stats': self.stats,
            'errors': self.errors
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(error_data, f, ensure_ascii=False, indent=2)
        
        self.logger.info(f"ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ä¿å­˜: {filename}")
    
    def process_talents(self, offset: int = 0, limit: int = 50):
        """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
        
        start_time = datetime.now()
        self.logger.info(f"ã‚¿ãƒ¬ãƒ³ãƒˆãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«å–å¾—é–‹å§‹ (ã‚ªãƒ•ã‚»ãƒƒãƒˆ: {offset}, ä»¶æ•°: {limit})")
        
        # å‡¦ç†å¯¾è±¡ã‚’å–å¾—
        talents = self.get_talents_to_process(offset, limit)
        self.stats['total'] = len(talents)
        
        # å„ã‚¿ãƒ¬ãƒ³ãƒˆã‚’å‡¦ç†
        for i, talent in enumerate(talents):
            talent_id = talent['talent_id']
            talent_name = talent['name']
            talent_link = talent['link']
            
            self.logger.info(f"[{i + 1}/{len(talents)}] å‡¦ç†ä¸­: {talent_name}")
            
            # ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«å–å¾—
            profile_data = self.scrape_talent_profile(talent_id, talent_link, talent_name)
            
            if profile_data:
                # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¿å­˜
                if self.save_profile(profile_data):
                    self.stats['success'] += 1
                else:
                    self.stats['failed'] += 1
            else:
                self.stats['failed'] += 1
            
            # ãƒ¬ãƒ¼ãƒˆåˆ¶é™
            time.sleep(1.5)
        
        # å®Œäº†å‡¦ç†
        end_time = datetime.now()
        execution_time = (end_time - start_time).total_seconds() / 60
        
        self.logger.info(f"å‡¦ç†å®Œäº†: {execution_time:.1f}åˆ†")
        self.logger.info(f"çµæœ: æˆåŠŸ {self.stats['success']}ä»¶, å¤±æ•— {self.stats['failed']}ä»¶")
        
        # ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ä¿å­˜
        self.save_error_log()
        
        # Discordé€šçŸ¥
        self.send_discord_notification()

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    
    import argparse
    
    parser = argparse.ArgumentParser(description='ã‚¿ãƒ¬ãƒ³ãƒˆãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«å–å¾—ï¼ˆä¿®æ­£ç‰ˆï¼‰')
    parser.add_argument('--mode', choices=['test', 'batch', 'full'], default='test',
                       help='å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰')
    parser.add_argument('--offset', type=int, default=0,
                       help='å‡¦ç†é–‹å§‹ã‚ªãƒ•ã‚»ãƒƒãƒˆ')
    
    args = parser.parse_args()
    
    # å‡¦ç†ä»¶æ•°ã‚’æ±ºå®š
    if args.mode == 'test':
        limit = 10
    elif args.mode == 'batch':
        limit = 50
    else:  # full
        limit = 1000
    
    # å‡¦ç†å®Ÿè¡Œ
    scraper = TalentProfileScraperFixed()
    scraper.process_talents(offset=args.offset, limit=limit)

if __name__ == "__main__":
    main()
